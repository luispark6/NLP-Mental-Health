{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "283a825a-8448-4e96-bb54-dfca2cd19aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0fdea83-0b8e-4c0b-9507-b5ef6d1ae91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        #does not pass all input words with eachother. Each word goes through independantly\n",
    "        #and the output are the embeddings of the word. We want this because we do not \n",
    "        #want to concacenate the embeddings to the output nodes.\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
    "        #now takes in all embeddings of each word stretched out\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.tokens = {}\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #embeds will be flattened matrix\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        #rectified relu to learn embeddings\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        #output is the log probablities of all vocabulary\n",
    "        return log_probs\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63bfd991-fc29-40b4-9c10-531edb2081aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3512\n",
      "Train Dataset size: 100\n",
      "Total Dictionary Size: 10,489\n",
      "Training Dictionary Size: 586\n",
      "Epoch: 0\n",
      "Total_Loss: 5931.0312304496765\n",
      "Epoch: 1\n",
      "Total_Loss: 4418.207268476486\n",
      "Epoch: 2\n",
      "Total_Loss: 3554.61175686121\n",
      "Epoch: 3\n",
      "Total_Loss: 2969.3809459507465\n",
      "Epoch: 4\n",
      "Total_Loss: 2433.111342936754\n",
      "Epoch: 5\n",
      "Total_Loss: 1901.7123607248068\n",
      "Epoch: 6\n",
      "Total_Loss: 1392.3067227303982\n",
      "Epoch: 7\n",
      "Total_Loss: 951.2950777262449\n",
      "Epoch: 8\n",
      "Total_Loss: 616.0986571870744\n",
      "Epoch: 9\n",
      "Total_Loss: 386.4205076172948\n"
     ]
    }
   ],
   "source": [
    "#file path to credit card csv file\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"mental_health.csv\")\n",
    "orig_dataset = pd.read_csv(file_path) #read csv file as pandas object\n",
    "orig_dataset = orig_dataset.to_numpy()\n",
    "print(\"Dataset size: \"+ str(len(orig_dataset)))\n",
    "dataset = orig_dataset[0:100] #use part of the dataset\n",
    "print(\"Train Dataset size: \"+ str(len(dataset)))\n",
    "minFreq = {} #word must appear n times to be added to dictionary\n",
    "dictionary = {} #relevant words in the dicationary\n",
    "index = 2\n",
    "for example in range(len(dataset)):\n",
    "    for cont_response in range(2):\n",
    "        if type(dataset[example][cont_response]) == float: #NaN values\n",
    "            continue\n",
    "        for word in dataset[example][cont_response].split():\n",
    "            if word not in minFreq:\n",
    "                minFreq[word]=1\n",
    "            else:\n",
    "                if minFreq[word]==3: #word needs to appear\n",
    "                    dictionary[word] = index\n",
    "                    index+=1\n",
    "                minFreq[word]+=1\n",
    "\n",
    "print( \"Total Dictionary Size: 10,489\")\n",
    "print(\"Training Dictionary Size: \" + str(index))\n",
    "\n",
    "CONTEXT_SIZE = 3 #look 3 words back to predict current word\n",
    "EMBEDDING_DIM = 252 #total embeddings for each word\n",
    "all_ngrams = [] #ngram setup -> [(['through', 'going', \"I'm\"], 'some')]\n",
    "for example in range(len(dataset)): \n",
    "    for cont_response in range(2): #context than response\n",
    "        if type(dataset[example][cont_response]) == float: #NaN values\n",
    "            continue\n",
    "        cur_Sentence = dataset[example][cont_response].split() #seperate by word\n",
    "        ngrams = [ #[(['through', 'going', \"I'm\"], 'some')]\n",
    "            ([cur_Sentence[i - j - 1] for j in range(CONTEXT_SIZE)],cur_Sentence[i])\n",
    "            for i in range(CONTEXT_SIZE, len(cur_Sentence))\n",
    "            ]\n",
    "        #append the grams to all_ngrams\n",
    "        for i in ngrams:\n",
    "            all_ngrams.append(i) \n",
    "loss_function = nn.NLLLoss() #loss layer\n",
    "model = NGramLanguageModeler(index, EMBEDDING_DIM, CONTEXT_SIZE) #intialize Ngram model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "model.tokens = dictionary\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    print(\"Epoch: \"+ str(epoch))\n",
    "    maxFreq = 3 #max number of times a word can be trained\n",
    "    #dictionary to keep track of times word is trained. Will skip if words have been trained maxFreq times\n",
    "    maxFreqDict = {}\n",
    "    for context, target in all_ngrams:\n",
    "        #if unknown word, just don't train\n",
    "        if context[0] not in dictionary or context[1] not in dictionary or context[2] not in dictionary:\n",
    "                continue\n",
    "        if target not in dictionary:\n",
    "                continue\n",
    "        #add context words if not found in dict\n",
    "        if context[0] not in maxFreqDict:\n",
    "            maxFreqDict[context[0]] = 1\n",
    "        if context[1] not in maxFreqDict:\n",
    "            maxFreqDict[context[1]] = 1\n",
    "        #if both words have been trained equal to or more than maxFreq times, continue\n",
    "        #already has been trained enough\n",
    "        if maxFreqDict[context[0]] >= maxFreq and maxFreqDict[context[1]] >= maxFreq:\n",
    "            continue\n",
    "        #update how many times the context words have been trained\n",
    "        maxFreqDict[context[0]]+=1\n",
    "        maxFreqDict[context[1]]+=1\n",
    "            \n",
    "        #turn each word to an integer and wrapped in tensor so pass as an input to the model\n",
    "        context_idxs = torch.tensor([dictionary[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        #zero out gradients cause it accumulates\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "        #apply the loss function to the log probabilties with the correct target word\n",
    "        loss = loss_function(log_probs, torch.tensor([dictionary[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Total_Loss: {total_loss}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87bdd6f4-bc7f-44b2-8dd7-218d78a5437e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"embedding_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3d7463e-6ccc-4042-ae90-636c10dcaec6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3156, -1.6006, -0.6120,  2.2860, -1.5039,  0.4343,  2.2365, -1.1154,\n",
      "          2.8599,  1.9770, -0.6094, -0.4991, -1.1724,  1.1033,  0.7778, -0.9378,\n",
      "         -1.7616,  0.7905,  0.3753,  0.3964, -0.1820,  0.4403,  1.5512, -0.3941,\n",
      "          0.3050,  0.3628, -0.3021, -0.1019, -0.3420,  0.3081,  0.8020,  0.2249,\n",
      "          1.8731, -1.4247, -0.6094, -0.3284,  0.6729, -0.3771, -0.4615,  0.1513,\n",
      "          0.1155,  1.7498,  1.5085, -0.9044, -1.3142,  0.3005, -1.0747,  1.7163,\n",
      "         -0.3107, -0.7557,  0.9453, -0.2776, -0.7360,  0.9223, -1.0980, -1.8236,\n",
      "         -0.3535,  1.6464, -0.6826,  0.0778, -0.5421, -0.3312,  1.6674, -0.5891,\n",
      "         -1.8353, -1.5515,  0.6684,  0.6346,  0.8659,  0.1948,  0.6650,  0.6090,\n",
      "          0.0421, -0.9157, -0.7937,  0.1100, -0.4822, -0.0822, -1.2967,  0.2717,\n",
      "         -0.8447,  0.4285,  0.4587, -1.6312,  1.5715, -0.0614, -0.0248,  1.4613,\n",
      "         -0.2493, -0.5492, -0.6738, -0.1331, -0.1685,  1.0556, -1.2171,  1.0258,\n",
      "         -0.7952, -1.0754, -0.1238, -1.9799,  0.1526, -0.8209,  0.9883, -0.0257,\n",
      "         -0.2758, -0.7390, -1.4239,  1.6192,  0.6102, -0.6748, -0.9839, -0.9160,\n",
      "          1.1191, -0.3376,  1.1877,  0.6521,  0.0678,  1.4610, -1.3270,  1.3824,\n",
      "         -0.2576,  0.7170,  0.1918,  0.6245, -0.5274,  0.0692, -0.3858,  0.9644,\n",
      "         -0.6150,  1.7352, -0.7942,  0.9737, -0.0474,  1.2267,  0.0040, -1.3506,\n",
      "         -0.9015,  0.7055, -0.4379,  2.0658,  0.5377,  0.5483,  0.0274, -1.2874,\n",
      "          0.8039,  0.3169, -0.1530, -0.1228, -0.3599,  0.3913,  0.6460, -1.9384,\n",
      "         -0.9912, -0.4227, -0.0270, -0.9903, -0.2102,  0.7368, -0.8518, -1.1892,\n",
      "          0.8198, -2.5591, -1.6558, -0.6459, -0.9872, -0.8313, -0.3943, -0.5779,\n",
      "         -0.4349,  1.4936,  0.9421,  0.8114, -1.6782,  0.3474, -0.6146,  1.6387,\n",
      "         -0.3479, -0.0196, -0.1561, -0.1806, -1.9006,  0.3176,  0.6958,  1.1343,\n",
      "          0.6433, -1.1598, -0.3326,  0.9577,  1.3935, -0.6111, -0.6461, -0.9419,\n",
      "          1.6382,  0.1215, -0.1742,  0.0689,  0.6681,  1.3067, -0.8161, -0.1137,\n",
      "          0.2318,  0.0460, -0.3186,  0.3371, -0.4201,  0.3599,  0.8773,  1.6666,\n",
      "         -0.1309,  1.1761,  0.0147,  0.9352, -0.2735, -0.4523,  0.3368, -0.2900,\n",
      "          0.1054, -0.2300, -0.4198,  0.2888,  0.8675,  1.0100,  0.6050, -2.2443,\n",
      "          1.6252,  1.3255, -0.7323,  1.2014,  0.5024, -0.5733, -2.0912, -1.0998,\n",
      "         -0.6165,  1.0598, -0.6540, -0.7750, -0.6172, -0.7364, -0.9883, -0.4630,\n",
      "         -0.5572, -1.2680,  1.0474,  0.1623,  0.0886, -0.4409,  0.2551, -0.0629,\n",
      "          0.5664,  0.3120,  0.9648,  0.4386]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.embeddings(torch.tensor([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "665631ae-ced6-46e4-98f6-fd8a159109f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(model.tokens, open(\"tokens.txt\",'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
