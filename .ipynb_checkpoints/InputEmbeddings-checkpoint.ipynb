{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283a825a-8448-4e96-bb54-dfca2cd19aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0fdea83-0b8e-4c0b-9507-b5ef6d1ae91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        #does not pass all input words with eachother. Each word goes through independantly\n",
    "        #and the output are the embeddings of the word. We want this because we do not \n",
    "        #want to concacenate the embeddings to the output nodes.\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
    "        #now takes in all embeddings of each word stretched out\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #embeds will be flattened matrix\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        #rectified relu to learn embeddings\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        #output is the log probablities of all vocabulary\n",
    "        return log_probs\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63bfd991-fc29-40b4-9c10-531edb2081aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22107\n",
      "0\n",
      "tensor([52, 48, 15])\n",
      "tensor([[ 7.4414e-01,  1.9966e-01, -1.4527e+00,  8.4134e-01, -7.4846e-01,\n",
      "         -7.8566e-01, -3.9399e-01, -2.3985e-01,  1.2674e+00,  7.2022e-01,\n",
      "         -4.9126e-02,  1.8048e+00, -1.3585e-01, -8.3153e-01, -2.5309e-01,\n",
      "          2.5834e+00,  2.9872e-01, -1.3039e+00,  1.2968e-01,  8.2620e-01,\n",
      "         -4.2135e-01,  1.5511e+00, -7.2142e-01, -6.2104e-01, -9.4118e-01,\n",
      "          1.1324e+00,  1.6325e+00,  1.2489e+00,  1.4183e+00, -2.1409e+00,\n",
      "          1.1068e+00,  2.0854e-01, -1.3033e+00,  3.6316e-01,  8.5487e-01,\n",
      "         -7.4163e-01, -7.6465e-01, -4.1204e-01,  1.2499e-01,  1.9339e+00,\n",
      "         -2.0929e-01, -9.9339e-01,  7.3446e-02,  1.2776e+00, -2.0305e+00,\n",
      "         -4.1967e-01, -8.2650e-01, -6.8663e-01,  1.8267e-01,  8.0684e-01,\n",
      "          7.2712e-01,  4.9817e-01,  1.1443e+00, -5.0324e-04, -1.0744e+00,\n",
      "          4.8676e-01, -2.8906e-01,  2.8544e+00,  1.0016e+00, -6.8231e-01,\n",
      "         -1.3146e+00, -1.0661e+00,  1.3817e+00,  1.3175e+00, -1.1066e-01,\n",
      "         -4.9795e-01,  1.4972e+00,  3.3064e-01,  8.2873e-02,  1.0011e+00,\n",
      "         -1.5112e-01, -7.5929e-01,  1.0104e+00, -1.4785e+00,  2.3974e+00,\n",
      "          1.9036e+00, -1.3918e+00, -7.9628e-01, -2.3788e-01, -5.5676e-01,\n",
      "          5.4509e-01,  4.8979e-01,  7.8822e-01, -2.9366e-01,  1.1211e+00,\n",
      "         -9.8884e-02, -1.1413e+00, -8.6596e-01, -1.0373e+00, -3.1426e-01,\n",
      "          8.8212e-01,  2.3508e+00, -1.3833e+00, -1.8100e+00,  1.6898e+00,\n",
      "         -1.7149e+00,  3.0111e-01, -7.3939e-01,  2.0103e-01,  1.3903e+00,\n",
      "         -2.8406e+00,  4.7359e-01,  2.1956e-01, -1.0072e+00, -5.5497e-01,\n",
      "          8.3769e-02, -3.0783e-01, -9.6548e-01,  4.9005e-02,  7.3510e-01,\n",
      "         -1.0132e+00, -5.1933e-01,  1.2470e-01, -2.1027e-02,  6.6989e-01,\n",
      "          1.1375e+00,  9.3810e-01,  2.0668e-01,  5.4321e-01, -6.7498e-01,\n",
      "          1.4579e-01,  1.7122e-02, -6.0125e-01,  3.1338e-01, -8.2769e-01,\n",
      "          3.4154e-01,  3.1067e-01, -6.7573e-01,  4.2583e-02, -1.2776e+00,\n",
      "          8.0951e-01, -6.8234e-01, -5.7127e-01,  9.8631e-01, -3.8604e-01,\n",
      "          1.3197e+00,  7.3026e-01,  2.2430e-01,  1.0804e+00,  6.2448e-01,\n",
      "          2.3288e+00,  4.9749e-01,  5.1397e-02,  1.6879e+00,  1.1157e+00,\n",
      "         -1.1435e+00,  3.5184e-01, -4.4929e-01,  2.1969e-01,  1.2026e+00,\n",
      "          6.0140e-01, -2.2753e+00,  8.2745e-01,  5.9800e-01,  7.3783e-01,\n",
      "         -1.7421e+00, -1.1669e+00,  1.7151e+00,  3.7592e-01, -9.0041e-01,\n",
      "          4.7723e-02, -2.9939e-01,  1.4331e+00,  1.3528e+00,  2.6369e-01,\n",
      "         -1.3077e-01, -1.4814e+00, -2.3584e+00, -7.5265e-01, -3.0459e-01,\n",
      "         -1.0467e-01, -7.7089e-01, -3.0278e-01,  5.3032e-01, -4.5923e-01,\n",
      "          8.1321e-01,  9.7762e-01,  6.8380e-01, -8.5806e-02,  1.3947e+00,\n",
      "         -6.0956e-01, -6.1122e-01,  3.1096e-01, -8.0179e-01,  1.6762e-01,\n",
      "          1.0569e+00, -4.3746e-01,  8.2969e-01,  1.2384e+00,  1.8517e+00,\n",
      "         -2.6168e-01, -2.2204e+00,  3.6142e-01,  2.4713e-01, -3.8002e-02,\n",
      "          1.0240e+00,  2.6749e-01,  1.9473e+00, -6.1585e-02,  1.5535e+00,\n",
      "         -8.8967e-03,  4.7896e-01, -2.5475e+00, -1.1756e+00,  1.0198e-01,\n",
      "         -1.9159e+00,  1.9648e+00,  1.4530e+00,  6.4033e-01, -3.0347e-01,\n",
      "         -3.3958e-02, -1.0221e+00,  3.1620e-01, -1.5596e+00,  9.8006e-01,\n",
      "         -1.1058e+00,  1.2611e+00, -2.2776e+00,  1.3645e+00,  2.1019e+00,\n",
      "          1.3139e+00, -8.7837e-01,  9.3612e-01, -1.3858e+00, -1.1156e-01,\n",
      "          4.2631e-01,  6.8297e-02, -5.3163e-01,  1.2876e+00,  7.1791e-01,\n",
      "         -1.3787e+00,  1.0050e-01,  9.8507e-01,  1.0531e-01, -6.8680e-01,\n",
      "          3.2109e-02, -5.5551e-01,  1.1160e+00, -2.6717e-01, -2.1681e-01,\n",
      "         -7.3782e-01,  1.4021e-01, -1.2326e+00,  4.3301e-01,  1.1617e+00,\n",
      "         -5.0671e-01,  2.1722e-01,  1.3789e+00, -5.3539e-01, -1.8903e-01,\n",
      "         -1.1777e+00,  1.2044e+00],\n",
      "        [-1.5818e+00, -1.3248e+00,  5.0613e-01, -1.5358e+00, -3.5359e-01,\n",
      "          9.1479e-01,  1.5947e-02,  1.4366e+00, -9.1871e-01, -8.0718e-01,\n",
      "          4.9757e-01,  8.0935e-01,  3.5358e-01,  1.8661e-01,  1.1458e+00,\n",
      "          3.0540e-02,  1.1133e+00,  6.8989e-01,  1.3450e+00, -2.4645e+00,\n",
      "          1.0692e+00, -2.2024e+00, -7.1099e-02,  2.6008e-01, -8.1217e-01,\n",
      "         -1.5478e+00,  1.1018e+00,  7.9139e-01,  1.1345e+00, -6.8032e-01,\n",
      "         -1.5182e+00, -1.0854e+00,  7.1755e-01, -3.9641e-01,  2.5032e-01,\n",
      "          1.3778e-01, -4.9085e-01,  6.6819e-01,  6.5846e-01, -1.1449e+00,\n",
      "         -1.1689e+00,  5.7371e-01,  2.4524e-01, -1.1101e+00, -5.7728e-02,\n",
      "         -8.6309e-01, -7.9560e-01, -2.6444e+00, -1.5392e+00,  2.3991e+00,\n",
      "         -9.9153e-01, -1.2819e+00, -1.2880e+00, -2.7344e+00,  1.3685e-01,\n",
      "          2.8116e-01,  1.2729e+00,  1.1487e+00, -1.1285e-01,  2.0440e-01,\n",
      "         -1.9978e+00, -2.0037e+00,  3.3953e-01, -1.7700e-01,  6.0177e-01,\n",
      "          1.0008e+00, -2.7036e-01, -7.9516e-01,  4.4956e-01, -4.1155e-01,\n",
      "          3.7898e-01, -4.3006e-01, -1.4319e+00,  9.6657e-01, -9.5406e-01,\n",
      "          2.7160e-01, -8.3012e-01,  1.8304e-01, -1.2045e+00, -5.5767e-01,\n",
      "         -1.1306e-01,  8.9654e-01, -7.6514e-01, -6.2487e-01, -9.5191e-01,\n",
      "          3.3096e-01,  1.0396e+00, -1.1460e+00,  6.4746e-01,  8.0093e-01,\n",
      "          3.1066e-02, -7.4214e-01,  1.6976e+00,  7.1036e-01,  2.0179e+00,\n",
      "          7.9372e-01, -5.5088e-01,  3.5353e-01,  1.4111e+00,  1.0775e+00,\n",
      "          5.3635e-01, -2.4024e-01, -2.9024e-01,  2.0008e+00,  1.4016e-01,\n",
      "         -1.4790e-01, -2.2534e-01, -2.1991e-01,  1.6153e-01, -4.4486e-01,\n",
      "          2.4185e-01, -1.6904e+00,  1.5524e-02, -1.4942e+00,  1.1817e+00,\n",
      "          4.6073e-01, -1.3365e+00, -5.7653e-01, -1.1281e-01,  1.6321e+00,\n",
      "         -1.2226e+00, -2.4173e-01,  2.4241e-01, -7.4389e-01,  6.2479e-01,\n",
      "         -5.1766e-01,  1.3976e+00,  1.3021e+00, -6.6649e-01, -8.1033e-01,\n",
      "          1.2211e-01,  4.4261e-01,  2.4762e-01,  2.9546e-02,  9.8341e-01,\n",
      "          1.7666e+00,  3.7760e-02,  2.1595e-01, -1.2195e+00,  4.4329e-01,\n",
      "          3.2184e-01, -1.6418e+00, -5.0862e-01, -7.7124e-01,  5.3593e-01,\n",
      "         -1.1970e-01, -6.5773e-01,  9.4762e-01, -9.8852e-01, -1.9485e+00,\n",
      "         -9.2205e-02,  1.0408e+00, -1.0944e+00, -3.2858e-01,  1.2730e+00,\n",
      "          6.6001e-01,  5.4071e-01,  2.3427e-01,  8.5004e-01,  4.4300e-01,\n",
      "          5.6193e-01,  1.8828e+00, -5.8627e-01,  1.6082e+00, -1.0564e-02,\n",
      "         -9.7186e-01,  1.1361e+00, -1.1648e+00, -4.2927e-02, -1.3774e+00,\n",
      "          2.1162e+00,  1.5651e+00,  9.2184e-02,  4.2437e-01, -1.4218e+00,\n",
      "         -7.4665e-01,  1.3062e+00,  1.1188e+00,  1.0235e+00,  1.8165e+00,\n",
      "          4.4696e-01,  1.2620e+00,  1.3643e+00,  1.4618e+00,  1.5401e+00,\n",
      "         -3.1909e-01,  2.0259e-01, -2.1988e-01,  4.8966e-01,  1.8175e-01,\n",
      "         -9.2688e-01, -3.8713e-02,  1.1891e+00, -1.3648e+00,  5.5843e-01,\n",
      "          9.1861e-01,  1.3908e+00,  9.1280e-01, -5.7506e-01,  5.2291e-01,\n",
      "         -4.8180e-01, -5.0169e-01, -1.8340e-01, -1.1038e+00, -6.1078e-01,\n",
      "          1.6973e-02, -2.0837e+00,  1.4603e+00,  3.3736e-01,  1.0273e+00,\n",
      "         -6.0623e-01, -2.5607e-01,  8.7881e-01, -7.6790e-01,  3.2338e-02,\n",
      "         -8.4495e-03, -1.1782e+00, -3.4006e-01, -1.5065e+00, -4.5019e-01,\n",
      "         -1.0939e+00,  1.4581e-01, -5.2431e-01, -9.6933e-01,  5.9019e-01,\n",
      "         -2.6360e-02, -5.5166e-01,  2.3064e-01,  5.5581e-02,  3.0857e-01,\n",
      "         -8.4641e-01, -2.3178e+00, -8.3727e-01,  7.1070e-01,  1.0064e+00,\n",
      "          2.1163e-01,  5.3158e-01, -2.2297e+00,  1.0353e-01,  5.3816e-01,\n",
      "          6.7213e-01, -8.5165e-01, -7.9594e-01,  5.2029e-01, -1.2335e+00,\n",
      "          2.5346e-01,  7.0960e-01, -1.1244e+00,  1.7127e-01,  7.5274e-01,\n",
      "          1.8643e+00, -8.6097e-01],\n",
      "        [-1.6742e-01,  8.4749e-01,  1.1141e-01, -7.8655e-01,  3.2583e-01,\n",
      "         -9.0356e-01,  3.8989e-01,  1.3451e+00, -7.0694e-01,  8.9937e-01,\n",
      "          1.5289e+00,  1.5011e-01, -1.9713e-01,  1.1678e+00, -1.8796e+00,\n",
      "          9.6600e-01,  5.9507e-01,  1.2195e+00, -9.5701e-01,  5.4622e-02,\n",
      "          1.2747e+00, -1.6900e+00,  4.2883e-01, -9.2487e-01,  1.8030e-01,\n",
      "         -6.6945e-01, -8.7649e-01, -1.3175e-01, -6.0054e-01,  5.9092e-01,\n",
      "          3.2375e-01,  7.2740e-01,  3.6509e-01, -3.1732e-01,  7.9411e-01,\n",
      "         -7.0672e-01,  8.6536e-01,  6.3480e-01, -1.4300e+00,  1.3748e+00,\n",
      "          9.0116e-01,  1.0224e+00, -4.8713e-01,  7.0964e-01,  1.6959e+00,\n",
      "          3.0993e-01,  8.6157e-01, -1.3114e-01, -9.3377e-01, -5.6033e-01,\n",
      "          1.7404e+00,  4.9555e-03, -2.3470e-01,  1.6154e+00,  8.9115e-01,\n",
      "          1.5018e+00, -7.9975e-01,  3.4983e-01, -1.2381e-01, -9.3213e-01,\n",
      "         -1.1617e+00, -5.9627e-01, -6.1160e-01,  1.4186e-01,  1.6589e-01,\n",
      "          3.1342e-01,  3.0142e-01, -2.0170e+00, -1.8009e+00, -9.3041e-01,\n",
      "         -4.9326e-01,  1.9327e-01, -1.2696e+00,  1.4260e+00,  1.6940e+00,\n",
      "         -1.1142e+00,  1.4739e+00,  6.6839e-02, -9.2597e-01, -5.4666e-01,\n",
      "         -4.7775e-01, -1.0102e-01, -1.7440e+00, -8.8447e-01, -8.2660e-02,\n",
      "          7.7761e-01,  8.6376e-01, -1.3549e+00, -1.2314e-01, -1.5211e+00,\n",
      "         -1.6312e+00,  2.0454e-01,  2.6298e-01,  1.6305e+00,  1.9052e+00,\n",
      "          9.5394e-01,  1.4381e+00,  4.6391e-01,  2.1063e-01,  1.9585e+00,\n",
      "          5.3390e-01, -3.1575e-02,  4.4938e-01,  2.8901e-01,  2.7617e-01,\n",
      "         -1.1160e+00,  2.6803e+00, -1.2512e+00,  1.2163e+00, -1.6559e-02,\n",
      "         -4.2607e-01,  3.6180e-01,  1.1719e+00, -1.5218e+00,  4.0875e-01,\n",
      "         -3.5847e-01,  6.3514e-01, -5.1949e-01,  5.4879e-01,  1.4006e+00,\n",
      "         -1.2118e+00, -2.9041e-01, -8.6731e-01,  1.2164e+00, -1.3691e-01,\n",
      "         -1.0882e-01, -2.7500e+00,  7.6386e-01, -1.6835e-01, -3.1957e-01,\n",
      "          7.6089e-01, -7.1132e-01,  4.8742e-02, -4.5035e-01, -1.0129e+00,\n",
      "         -1.3996e-01,  4.2966e-01,  6.5016e-01,  1.6535e-01,  1.5702e-01,\n",
      "          7.2813e-01,  5.1101e-01,  6.7130e-01,  5.2341e-01,  1.2460e+00,\n",
      "          3.8022e-01,  1.0307e+00, -1.9776e-01,  5.1999e-01,  7.8975e-01,\n",
      "         -8.8281e-01, -2.3095e-01,  1.7302e+00,  2.3617e-01,  3.2625e-01,\n",
      "          4.6408e-01, -7.5218e-01, -8.2377e-01, -1.6708e-01,  4.6431e-01,\n",
      "          4.0803e-01,  7.7262e-01, -1.2200e-01,  5.1053e-01,  8.2542e-01,\n",
      "         -1.1998e+00, -1.5493e+00, -1.4568e-01,  1.1733e+00,  2.9396e-02,\n",
      "         -1.3568e+00, -7.7978e-01, -8.1670e-01, -1.4450e+00,  8.3195e-01,\n",
      "         -4.8328e-01,  9.6248e-03,  1.0947e+00,  5.4832e-03,  6.9167e-01,\n",
      "          9.2274e-01, -8.4435e-01, -1.1685e+00,  1.2294e+00,  9.0493e-01,\n",
      "          1.1750e+00,  3.4144e-03, -1.4602e+00,  7.8749e-01, -1.8471e+00,\n",
      "          5.4783e-01,  5.2691e-01,  1.0430e-01, -1.1283e+00, -8.5772e-01,\n",
      "         -1.3920e+00,  7.2066e-01,  1.9992e-01, -9.5074e-01,  2.9217e-01,\n",
      "         -1.2125e+00,  1.8922e+00, -2.0119e-01, -2.8798e-01, -8.1765e-01,\n",
      "         -1.8556e-01, -1.3182e+00, -7.0747e-01,  6.1007e-01,  4.1670e-01,\n",
      "          6.7053e-01,  1.7467e-01,  1.8057e+00, -1.7813e+00, -1.7942e-01,\n",
      "         -3.8000e-01, -1.1321e+00,  1.5593e+00,  1.6777e-01, -1.7441e+00,\n",
      "          1.1657e+00,  6.8924e-01, -1.3528e+00,  9.3898e-01,  2.7606e-01,\n",
      "          4.3337e-01,  1.8074e-01, -1.8129e+00, -5.3860e-01, -4.9277e-01,\n",
      "          8.6211e-01,  1.2509e-01,  6.8953e-01,  7.8253e-01,  1.8180e-01,\n",
      "         -2.0866e+00, -7.9721e-01, -1.1635e+00, -1.6918e-01,  4.9411e-01,\n",
      "         -1.3552e+00, -4.8442e-01, -7.3936e-01,  4.7070e-01,  1.3516e+00,\n",
      "         -1.2205e+00,  9.6443e-01, -4.5196e-01, -1.9185e-01,  1.4464e-01,\n",
      "         -4.1598e-01,  1.3476e-01]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x252 and 756x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 66\u001b[0m\n\u001b[1;32m     62\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Step 3. Run the forward pass, getting log probabilities over next\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# words\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m model(context_idxs)\n\u001b[1;32m     69\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(log_probs, dim\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     70\u001b[0m predicted_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(predicted_classes[\u001b[38;5;241m1\u001b[39m]) \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m, in \u001b[0;36mNGramLanguageModeler.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(embeds)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#rectified relu to learn embeddings\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(embeds))\n\u001b[1;32m     18\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(out)\n\u001b[1;32m     19\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x252 and 756x128)"
     ]
    }
   ],
   "source": [
    "#file path to credit card csv file\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"mental_health.csv\")\n",
    "dataset = pd.read_csv(file_path) #read csv file as pandas object\n",
    "dataset = dataset.to_numpy()\n",
    "dataset = dataset[0:len(dataset)//30] #use part of the dataset\n",
    "\n",
    "\n",
    "minFreq = {} #word must appear n times to be added to dictionary\n",
    "dictionary = {} #relevant words in the dicationary\n",
    "index = 2\n",
    "for example in range(len(dataset)):\n",
    "    for cont_response in range(2):\n",
    "        if type(dataset[example][cont_response]) == float: #NaN values\n",
    "            continue\n",
    "        for word in dataset[example][cont_response].split():\n",
    "            if word not in minFreq:\n",
    "                minFreq[word]=1\n",
    "            else:\n",
    "                if minFreq[word]==3: #word needs to appear\n",
    "                    dictionary[word] = index\n",
    "                    index+=1\n",
    "                minFreq[word]+=1\n",
    "                \n",
    "CONTEXT_SIZE = 3 #look 3 words back to predict current word\n",
    "EMBEDDING_DIM = 252 #total embeddings for each word\n",
    "all_ngrams = [] #ngram setup -> [(['through', 'going', \"I'm\"], 'some')]\n",
    "for example in range(len(dataset)): \n",
    "    for cont_response in range(2): #context than response\n",
    "        if type(dataset[example][cont_response]) == float: #NaN values\n",
    "            continue\n",
    "        cur_Sentence = dataset[example][cont_response].split() #seperate by word\n",
    "        ngrams = [ #[(['through', 'going', \"I'm\"], 'some')]\n",
    "            ([cur_Sentence[i - j - 1] for j in range(CONTEXT_SIZE)],cur_Sentence[i])\n",
    "            for i in range(CONTEXT_SIZE, len(cur_Sentence))\n",
    "            ]\n",
    "        #append the grams to all_ngrams\n",
    "        for i in ngrams:\n",
    "            all_ngrams.append(i)\n",
    "\n",
    "loss_function = nn.NLLLoss() #loss layer\n",
    "model = NGramLanguageModeler(index, EMBEDDING_DIM, CONTEXT_SIZE) #intialize Ngram model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001) #use adam optimizer\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    total  = 0\n",
    "    correct = 0\n",
    "    print(epoch)\n",
    "    for context, target in all_ngrams:\n",
    "        #if unknown word, just don't train\n",
    "        if context[0] not in dictionary or context[1] not in dictionary or context[2] not in dictionary:\n",
    "                continue\n",
    "        if target not in dictionary:\n",
    "                continue\n",
    "        #turn each word to an integer and wrapped in tensor so pass as an input to the model\n",
    "        context_idxs = torch.tensor([dictionary[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        #zero out gradients cause it accumulates\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "        \n",
    "        #the predicted class will be the max log probability\n",
    "        predicted_class = torch.max(log_probs, dim= 1)\n",
    "        predicted_index = int(predicted_class[1])  #convert to int\n",
    "        #accumulate correct predictions\n",
    "        if predicted_index == dictionary[target]:\n",
    "            correct+=1\n",
    "            \n",
    "        \n",
    "\n",
    "        #apply the loss function to the log probabilties with the correct target word\n",
    "        loss = loss_function(log_probs, torch.tensor([dictionary[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        total+=1\n",
    "    print(f\"Accuracy: {correct/total}\")\n",
    "    print(f\"Total_Loss: {total_loss}\")\n",
    "\n",
    "    \n",
    "test = dataset[(len(dataset)//30)+1:(len(dataset)//30)+ 10000]\n",
    "test_grams = []\n",
    "for example in range(len(test)):\n",
    "    for cont_response in range(2):\n",
    "        if type(test[example][cont_response]) == float:\n",
    "            continue\n",
    "        cur_Sentence = test[example][cont_response].split()\n",
    "        sngrams = [\n",
    "            ([cur_Sentence[i - j - 1] for j in range(CONTEXT_SIZE)],cur_Sentence[i])\n",
    "            for i in range(CONTEXT_SIZE, len(cur_Sentence))\n",
    "            ]\n",
    "        for i in sngrams:\n",
    "            test_grams.append(i)\n",
    "      \n",
    "    \n",
    "    \n",
    "total = 0\n",
    "correct = 0\n",
    "for context, target in test_grams:\n",
    "    i+=1\n",
    "    # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "    # into integer indices and wrap them in tensors)\n",
    "    if context[0] not in dictionary or context[1] not in dictionary or context[2] not in dictionary:\n",
    "            continue\n",
    "    if target not in dictionary:\n",
    "            continue\n",
    "\n",
    "    context_idxs = torch.tensor([dictionary[w] for w in context], dtype=torch.long)\n",
    "\n",
    "\n",
    "    # Step 3. Run the forward pass, getting log probabilities over next\n",
    "    # words\n",
    "    log_probs = model(context_idxs)\n",
    "    predicted_classes = torch.max(log_probs, dim= 1)\n",
    "    predicted_index = int(predicted_classes[1]) \n",
    "    if predicted_index == dictionary[target]:\n",
    "        correct+=1\n",
    "\n",
    "\n",
    "\n",
    "    # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "    # word wrapped in a tensor)\n",
    "    loss = loss_function(log_probs, torch.tensor([dictionary[target]], dtype=torch.long))\n",
    "    # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "    total_loss += loss.item()\n",
    "    total+=1\n",
    "print(f\"Accuracy: {correct/total}\")\n",
    "print(f\"Total_Loss: {total_loss}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bdd6f4-bc7f-44b2-8dd7-218d78a5437e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
