{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283a825a-8448-4e96-bb54-dfca2cd19aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0fdea83-0b8e-4c0b-9507-b5ef6d1ae91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        #does not pass all input words with eachother. Each word goes through independantly\n",
    "        #and the output are the embeddings of the word. We want this because we do not \n",
    "        #want to concacenate the embeddings to the output nodes.\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
    "        #now takes in all embeddings of each word stretched out\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.tokens = {}\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #embeds will be flattened matrix\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        #rectified relu to learn embeddings\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        #output is the log probablities of all vocabulary\n",
    "        return log_probs\n",
    "    def multiply_embedding_weights(self):\n",
    "        # Multiply the weights of the embedding layer by sqrt(embedding_dim)\n",
    "        self.embeddings.weight.data = self.embeddings.weight.data * (self.embedding_dim ** 0.5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63bfd991-fc29-40b4-9c10-531edb2081aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3512\n",
      "Train Dataset size: 100\n",
      "Total Dictionary Size: 10,489\n",
      "Training Dictionary Size: 589\n",
      "Epoch: 0\n",
      "Total_Loss: 5906.405691623688\n",
      "Epoch: 1\n",
      "Total_Loss: 4417.181990623474\n",
      "Epoch: 2\n",
      "Total_Loss: 3579.4913625121117\n",
      "Epoch: 3\n",
      "Total_Loss: 3001.517051100731\n",
      "Epoch: 4\n",
      "Total_Loss: 2464.736441999674\n",
      "Epoch: 5\n",
      "Total_Loss: 1930.116067290306\n",
      "Epoch: 6\n",
      "Total_Loss: 1413.5676435604692\n",
      "Epoch: 7\n",
      "Total_Loss: 961.7724005840719\n",
      "Epoch: 8\n",
      "Total_Loss: 616.9519104510546\n",
      "Epoch: 9\n",
      "Total_Loss: 381.9774593450129\n"
     ]
    }
   ],
   "source": [
    "#file path to credit card csv file\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"mental_health.csv\")\n",
    "orig_dataset = pd.read_csv(file_path) #read csv file as pandas object\n",
    "orig_dataset = orig_dataset.to_numpy()\n",
    "print(\"Dataset size: \"+ str(len(orig_dataset)))\n",
    "dataset = orig_dataset[0:100] #use part of the dataset\n",
    "print(\"Train Dataset size: \"+ str(len(dataset)))\n",
    "minFreq = {} #word must appear n times to be added to dictionary\n",
    "dictionary = {} #relevant words in the dicationary\n",
    "index = 5\n",
    "for example in range(len(dataset)):\n",
    "    for cont_response in range(2):\n",
    "        if type(dataset[example][cont_response]) == float: #NaN values\n",
    "            continue\n",
    "        for word in dataset[example][cont_response].split():\n",
    "            if word not in minFreq:\n",
    "                minFreq[word]=1\n",
    "            else:\n",
    "                if minFreq[word]==3: #word needs to appear\n",
    "                    dictionary[word] = index\n",
    "                    index+=1\n",
    "                minFreq[word]+=1\n",
    "\n",
    "print( \"Total Dictionary Size: 10,489\")\n",
    "print(\"Training Dictionary Size: \" + str(index))\n",
    "\n",
    "CONTEXT_SIZE = 3 #look 3 words back to predict current word\n",
    "EMBEDDING_DIM = 252 #total embeddings for each word\n",
    "all_ngrams = [] #ngram setup -> [(['through', 'going', \"I'm\"], 'some')]\n",
    "for example in range(len(dataset)): \n",
    "    for cont_response in range(2): #context than response\n",
    "        if type(dataset[example][cont_response]) == float: #NaN values\n",
    "            continue\n",
    "        cur_Sentence = dataset[example][cont_response].split() #seperate by word\n",
    "        ngrams = [ #[(['through', 'going', \"I'm\"], 'some')]\n",
    "            ([cur_Sentence[i - j - 1] for j in range(CONTEXT_SIZE)],cur_Sentence[i])\n",
    "            for i in range(CONTEXT_SIZE, len(cur_Sentence))\n",
    "            ]\n",
    "        #append the grams to all_ngrams\n",
    "        for i in ngrams:\n",
    "            all_ngrams.append(i) \n",
    "loss_function = nn.NLLLoss() #loss layer\n",
    "model = NGramLanguageModeler(index, EMBEDDING_DIM, CONTEXT_SIZE) #intialize Ngram model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "model.tokens = dictionary\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    print(\"Epoch: \"+ str(epoch))\n",
    "    maxFreq = 3 #max number of times a word can be trained\n",
    "    #dictionary to keep track of times word is trained. Will skip if words have been trained maxFreq times\n",
    "    maxFreqDict = {}\n",
    "    for context, target in all_ngrams:\n",
    "        #if unknown word, just don't train\n",
    "        if context[0] not in dictionary or context[1] not in dictionary or context[2] not in dictionary:\n",
    "                continue\n",
    "        if target not in dictionary:\n",
    "                continue\n",
    "        #add context words if not found in dict\n",
    "        if context[0] not in maxFreqDict:\n",
    "            maxFreqDict[context[0]] = 1\n",
    "        if context[1] not in maxFreqDict:\n",
    "            maxFreqDict[context[1]] = 1\n",
    "        #if both words have been trained equal to or more than maxFreq times, continue\n",
    "        #already has been trained enough\n",
    "        if maxFreqDict[context[0]] >= maxFreq and maxFreqDict[context[1]] >= maxFreq:\n",
    "            continue\n",
    "        #update how many times the context words have been trained\n",
    "        maxFreqDict[context[0]]+=1\n",
    "        maxFreqDict[context[1]]+=1\n",
    "            \n",
    "        #turn each word to an integer and wrapped in tensor so pass as an input to the model\n",
    "        context_idxs = torch.tensor([dictionary[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        #zero out gradients cause it accumulates\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "        #apply the loss function to the log probabilties with the correct target word\n",
    "        loss = loss_function(log_probs, torch.tensor([dictionary[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Total_Loss: {total_loss}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87bdd6f4-bc7f-44b2-8dd7-218d78a5437e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.multiply_embedding_weights()\n",
    "torch.save(model.state_dict(), \"embedding_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3d7463e-6ccc-4042-ae90-636c10dcaec6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 18.9515, -40.1793,  -3.4174,  35.4660,  -0.3803,  13.8064, -16.4588,\n",
      "           9.0106,  13.3778,  28.5099,  -2.3675,  17.4134,  20.8600, -11.6594,\n",
      "          14.0289,  13.5806,  18.0991,  -2.5940,  -2.5052, -16.3638,  21.1693,\n",
      "           4.1400, -15.8711, -10.0324,  -6.1070,   7.8133,  24.1666, -23.8496,\n",
      "         -11.6094,  -6.6620,  -6.2208, -31.4289,  -3.7220,  31.1608,  17.8560,\n",
      "         -21.7332,   4.6913,  20.6389,  -2.3212, -12.1803,   0.1400, -10.8554,\n",
      "          -6.3194,  -4.5123,  -9.4419,  14.5700,  10.3055,   7.2209,   6.9968,\n",
      "         -19.7591,  -7.0838,   6.0305, -17.4756, -10.1409,   2.0908,  -1.6202,\n",
      "         -13.1677, -30.9968, -28.6185,  10.8681,  15.4581, -15.5274,  31.7097,\n",
      "         -23.2060,   2.2888, -17.3552,  16.2699, -16.3184,  19.5036, -17.8487,\n",
      "          13.4372,   3.8102, -12.5416,  -0.8427,   8.9910,   0.2782,   7.9117,\n",
      "           4.7300, -15.8522, -22.5057,   7.8781,  -0.9336,  28.2551,  14.6805,\n",
      "          -9.8760,  23.1537,   3.0672,   5.6760,  -6.2642, -17.2485,   6.8457,\n",
      "           6.8063,   9.4308,  -3.2454, -15.1117,  -9.7042,  10.7670,  15.9843,\n",
      "           6.0357,   8.3157, -11.0909,   9.1473,   5.6264,   7.0035, -19.3233,\n",
      "          -8.0756, -14.3773,   4.5420, -17.0634,   2.9442,  16.0694, -34.1821,\n",
      "         -14.0409,  11.0377, -24.3346,   3.3643,  16.0093, -21.1309,   4.2339,\n",
      "           7.1234,  16.3607,  25.9471,   0.5941, -16.1822,  -1.7221,  31.0068,\n",
      "           1.7770, -23.0911,  33.2411,  -3.0109,  10.3224,  22.0745, -15.8026,\n",
      "          -8.7355,  19.7215,  22.1539, -14.2708,  -2.1528,   1.6106,  -6.8839,\n",
      "          14.5486,   8.8570,   0.9597,   0.5442,   6.6979,   6.2106,   8.0096,\n",
      "         -24.5184,  18.7840,  -1.0004,  -2.2332,  -2.0907,   5.9642, -25.4986,\n",
      "          -9.2781, -14.6941,  16.7510,   8.0534,   8.6975,   0.9448, -12.9366,\n",
      "          13.6099, -14.0059,   3.1369,  12.7691,   4.6873,   8.4185, -14.4798,\n",
      "          -2.1503,  16.2339,  18.0450, -18.8973,  13.2842,  21.4721,   2.0450,\n",
      "         -10.7688, -13.3177, -15.0060,  18.7700,  -0.1628, -11.6157,   4.0927,\n",
      "          27.0457,   2.1387,  19.8380, -22.6531, -29.5582,  -8.7295,  -1.8986,\n",
      "         -12.5929,  -9.1556,  -0.4360,  -3.7910, -14.3987,  -3.1311,   5.9513,\n",
      "           1.6372, -23.9275,  36.1528,  17.3757, -12.7001,   8.0833,  17.8407,\n",
      "          18.7228,  16.2115,  11.2025,   1.1244, -13.1283, -14.9246,  27.0747,\n",
      "          17.3781,   4.3795, -13.7068,   4.2872,  -9.4306,   0.6295,  10.7926,\n",
      "         -11.7280,  -8.6431,  10.4693,  15.4701,   9.7502, -15.4389,  30.3401,\n",
      "         -39.2284,  -4.3552,  31.0793,   1.8792,   9.9994, -10.5536,  26.8531,\n",
      "           7.0998,  14.2940,   6.2325,  24.2323,  -0.5682,  -9.9535,  15.6242,\n",
      "           9.9782,  -0.2818,  28.6031, -15.5568, -30.7714, -16.3194,  17.3452,\n",
      "          -0.9154, -22.1983, -15.1852,  20.9084,  -0.6857,  -6.2808,  10.4059]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.embeddings(torch.tensor([0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "665631ae-ced6-46e4-98f6-fd8a159109f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(model.tokens, open(\"tokens.txt\",'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
