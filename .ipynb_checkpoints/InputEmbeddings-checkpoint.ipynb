{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "283a825a-8448-4e96-bb54-dfca2cd19aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0fdea83-0b8e-4c0b-9507-b5ef6d1ae91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        #does not pass all input words with eachother. Each word goes through independantly\n",
    "        #and the output are the embeddings of the word. We want this because we do not \n",
    "        #want to concacenate the embeddings to the output nodes.\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
    "        #now takes in all embeddings of each word stretched out\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.tokens = {}\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #embeds will be flattened matrix\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        #rectified relu to learn embeddings\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        #output is the log probablities of all vocabulary\n",
    "        return log_probs\n",
    "    def multiply_embedding_weights(self):\n",
    "        # Multiply the weights of the embedding layer by sqrt(embedding_dim)\n",
    "        self.embeddings.weight.data = self.embeddings.weight.data * (self.embedding_dim ** 0.5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63bfd991-fc29-40b4-9c10-531edb2081aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3512\n",
      "Train Dataset size: 1800\n",
      "Total Dictionary Size: 10,489\n",
      "Training Dictionary Size: 6764\n",
      "Epoch: 0\n",
      "Total_Loss: 86064.09328246117\n",
      "Epoch: 1\n",
      "Total_Loss: 66810.90182030201\n",
      "Epoch: 2\n",
      "Total_Loss: 59706.972353339195\n",
      "Epoch: 3\n",
      "Total_Loss: 53588.15667644143\n",
      "Epoch: 4\n",
      "Total_Loss: 47330.66776663065\n",
      "Epoch: 5\n",
      "Total_Loss: 40343.34147899598\n",
      "Epoch: 6\n",
      "Total_Loss: 32701.35385359265\n",
      "Epoch: 7\n",
      "Total_Loss: 25236.627613145974\n",
      "Epoch: 8\n",
      "Total_Loss: 19042.636997162088\n",
      "Epoch: 9\n",
      "Total_Loss: 14301.982699767528\n",
      "Epoch: 10\n",
      "Total_Loss: 10654.864692992236\n",
      "Epoch: 11\n",
      "Total_Loss: 7822.320937789023\n",
      "Epoch: 12\n",
      "Total_Loss: 5592.175120540771\n",
      "Epoch: 13\n",
      "Total_Loss: 3853.5923677406117\n",
      "Epoch: 14\n",
      "Total_Loss: 2545.3893638989493\n",
      "Epoch: 15\n",
      "Total_Loss: 1609.7522348984137\n",
      "Epoch: 16\n",
      "Total_Loss: 975.1481316014143\n",
      "Epoch: 17\n",
      "Total_Loss: 559.2464330844164\n",
      "Epoch: 18\n",
      "Total_Loss: 325.04877618473165\n",
      "Epoch: 19\n",
      "Total_Loss: 192.66555631486693\n",
      "Epoch: 20\n",
      "Total_Loss: 133.90883394184667\n",
      "Epoch: 21\n",
      "Total_Loss: 99.52917654457431\n",
      "Epoch: 22\n",
      "Total_Loss: 86.26693473287023\n",
      "Epoch: 23\n",
      "Total_Loss: 81.34212253470346\n",
      "Epoch: 24\n",
      "Total_Loss: 71.3304038827724\n"
     ]
    }
   ],
   "source": [
    "#file path to credit card csv file\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"mental_health.csv\")\n",
    "orig_dataset = pd.read_csv(file_path) #read csv file as pandas object\n",
    "orig_dataset = orig_dataset.to_numpy()\n",
    "print(\"Dataset size: \"+ str(len(orig_dataset)))\n",
    "dataset = orig_dataset[np.random.choice(orig_dataset.shape[0], size=1800, replace=True)]\n",
    "print(\"Train Dataset size: \"+ str(len(dataset)))\n",
    "minFreq = {} #word must appear n times to be added to dictionary\n",
    "dictionary = {} #relevant words in the dicationary\n",
    "index = 5\n",
    "for example in range(len(dataset)):\n",
    "    for cont_response in range(2):\n",
    "        if type(dataset[example][cont_response]) == float: #NaN values\n",
    "            continue\n",
    "        for word in dataset[example][cont_response].split():\n",
    "            if word not in minFreq:\n",
    "                minFreq[word]=1\n",
    "            else:\n",
    "                if minFreq[word]==3: #word needs to appear\n",
    "                    dictionary[word] = index\n",
    "                    index+=1\n",
    "                minFreq[word]+=1\n",
    "\n",
    "print( \"Total Dictionary Size: 10,489\")\n",
    "print(\"Training Dictionary Size: \" + str(index))\n",
    "\n",
    "CONTEXT_SIZE = 3 #look 3 words back to predict current word\n",
    "EMBEDDING_DIM = 252 #total embeddings for each word\n",
    "all_ngrams = [] #ngram setup -> [(['through', 'going', \"I'm\"], 'some')]\n",
    "for example in range(len(dataset)): \n",
    "    for cont_response in range(2): #context than response\n",
    "        if type(dataset[example][cont_response]) == float: #NaN values\n",
    "            continue\n",
    "        cur_Sentence = dataset[example][cont_response].split() #seperate by word\n",
    "        ngrams = [ #[(['through', 'going', \"I'm\"], 'some')]\n",
    "            ([cur_Sentence[i - j - 1] for j in range(CONTEXT_SIZE)],cur_Sentence[i])\n",
    "            for i in range(CONTEXT_SIZE, len(cur_Sentence))\n",
    "            ]\n",
    "        #append the grams to all_ngrams\n",
    "        for i in ngrams:\n",
    "            all_ngrams.append(i) \n",
    "loss_function = nn.NLLLoss() #loss layer\n",
    "model = NGramLanguageModeler(index, EMBEDDING_DIM, CONTEXT_SIZE) #intialize Ngram model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "model.tokens = dictionary\n",
    "for epoch in range(25):\n",
    "    total_loss = 0\n",
    "    print(\"Epoch: \"+ str(epoch))\n",
    "    maxFreq = 3 #max number of times a word can be trained\n",
    "    #dictionary to keep track of times word is trained. Will skip if words have been trained maxFreq times\n",
    "    maxFreqDict = {}\n",
    "    for context, target in all_ngrams:\n",
    "        #if unknown word, just don't train\n",
    "        if context[0] not in dictionary or context[1] not in dictionary or context[2] not in dictionary:\n",
    "                continue\n",
    "        if target not in dictionary:\n",
    "                continue\n",
    "        #add context words if not found in dict\n",
    "        if context[0] not in maxFreqDict:\n",
    "            maxFreqDict[context[0]] = 1\n",
    "        if context[1] not in maxFreqDict:\n",
    "            maxFreqDict[context[1]] = 1\n",
    "        #if both words have been trained equal to or more than maxFreq times, continue\n",
    "        #already has been trained enough\n",
    "        if maxFreqDict[context[0]] >= maxFreq and maxFreqDict[context[1]] >= maxFreq:\n",
    "            continue\n",
    "        #update how many times the context words have been trained\n",
    "        maxFreqDict[context[0]]+=1\n",
    "        maxFreqDict[context[1]]+=1\n",
    "            \n",
    "        #turn each word to an integer and wrapped in tensor so pass as an input to the model\n",
    "        context_idxs = torch.tensor([dictionary[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        #zero out gradients cause it accumulates\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "        #apply the loss function to the log probabilties with the correct target word\n",
    "        loss = loss_function(log_probs, torch.tensor([dictionary[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Total_Loss: {total_loss}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87bdd6f4-bc7f-44b2-8dd7-218d78a5437e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.multiply_embedding_weights()\n",
    "torch.save(model.state_dict(), \"embedding_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d7463e-6ccc-4042-ae90-636c10dcaec6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.3159e+01,  3.0916e+01,  1.0142e+01, -1.1338e+01, -3.0613e+01,\n",
      "          5.3573e+00,  7.1293e+00, -2.9001e+00, -2.6006e+01, -3.9583e+00,\n",
      "          4.8073e+00,  2.4731e+01, -1.4453e+01,  2.4413e+01, -2.2181e+01,\n",
      "         -7.6317e+00, -4.5061e+00,  1.0441e+01, -1.8528e+01, -1.2620e+01,\n",
      "          1.3454e+01, -3.3704e+01, -2.6307e+01, -9.2184e-01, -5.5030e+00,\n",
      "          2.0827e+00, -1.7685e+01, -2.0907e+01,  1.5917e+01, -2.7964e+01,\n",
      "          4.3684e+00,  2.0318e+01,  8.3701e+00,  4.7877e+00, -1.4974e+01,\n",
      "         -3.6098e+00, -1.8023e+01,  1.6274e+00, -1.2133e+01, -2.7384e+00,\n",
      "          6.8451e+00, -2.6368e+01,  1.6887e+01,  2.6482e+00, -3.1264e+01,\n",
      "          3.7697e+01,  1.6542e+01,  5.6029e+00, -9.3785e+00, -6.3943e+00,\n",
      "          1.0949e+01,  1.4152e+01, -6.4111e+00, -7.7125e+00,  1.7517e+01,\n",
      "         -2.2049e+00, -2.2604e+00,  6.4946e+00, -1.1784e+01,  4.0612e-01,\n",
      "         -2.6253e+00,  3.8961e+00, -2.6074e+00, -1.8644e+01,  6.4926e+00,\n",
      "          8.2810e+00,  2.0053e+01, -8.5754e-01,  1.4489e+01,  6.0151e-01,\n",
      "         -4.8548e+00, -7.5694e+00, -1.5705e+01,  3.2989e+01, -2.3795e+01,\n",
      "          1.1434e+01,  1.1627e+01, -1.8788e+01, -1.4896e+01, -1.2848e+01,\n",
      "         -2.1719e+00, -6.6278e+00,  1.2414e+01,  2.8285e+01, -1.0586e+01,\n",
      "          1.4666e+01, -1.9903e+01,  2.8043e+01, -3.5394e+00,  2.7928e+00,\n",
      "         -3.7818e+00,  1.7708e+01, -2.2119e+01,  7.2874e+00,  1.9734e+01,\n",
      "         -2.7597e-01, -1.1432e+01,  2.8513e+00, -1.4574e+01, -1.2200e+00,\n",
      "          1.6580e+01,  4.1159e+01,  2.0774e+00, -1.2034e+01,  3.1788e+01,\n",
      "         -1.4125e+01, -3.9570e+01,  1.9730e+01, -8.1863e+00,  1.3836e+01,\n",
      "          1.0962e+01,  1.3862e+01,  2.6891e+01, -7.0906e+00, -1.4999e+00,\n",
      "          3.5012e+00, -2.5951e+01,  2.2981e-03,  8.5361e+00,  4.7796e+00,\n",
      "         -7.3277e+00,  5.3395e-01, -1.1692e+00, -9.7817e+00,  2.1855e+00,\n",
      "          6.5558e+00, -3.8256e+01,  1.6034e+01,  1.7821e+01, -1.3635e+01,\n",
      "         -1.3282e+00, -1.3029e+01,  1.5406e+01, -5.5387e+00,  5.6640e+00,\n",
      "         -1.1287e+01,  1.2535e+01, -8.0826e+00,  2.2455e+01,  2.4744e+00,\n",
      "         -1.2336e+01, -4.8730e+00,  4.2535e+00,  1.3205e+01, -2.1983e+01,\n",
      "         -1.4980e+01, -5.3851e-01, -2.5902e+01,  3.0369e+01, -1.8514e+00,\n",
      "         -3.2981e+01,  4.7529e+00,  2.3037e+00,  2.1635e-01, -1.3136e+00,\n",
      "          7.9448e+00, -2.2844e+01, -8.7919e+00,  3.6281e+00,  1.7648e+01,\n",
      "          8.7603e+00,  1.3218e+01,  1.2987e+01, -1.8901e+01,  6.7591e+00,\n",
      "          4.3974e-01,  3.2487e+01, -2.7501e+01,  1.0868e+01,  4.2486e+00,\n",
      "         -6.5625e+00, -3.6129e+01,  9.7869e+00, -1.8832e+00, -1.7699e+01,\n",
      "         -2.5103e-01, -7.8518e+00, -3.4221e+00,  5.2436e+01,  1.3160e+01,\n",
      "         -9.0202e+00,  6.7811e+00, -1.6046e+00, -6.1902e+00, -1.4029e+01,\n",
      "         -2.0288e+01, -2.7060e+01, -1.4792e+00,  3.1471e+01,  1.0912e+01,\n",
      "         -4.5078e+00, -5.1081e+00,  1.7623e+01,  7.8593e+00,  2.6300e+01,\n",
      "         -4.7227e+00, -1.7071e+01, -1.9623e+01,  8.2392e+00, -6.4080e+00,\n",
      "         -9.5383e-02, -1.9886e+01,  1.7929e+00, -1.0837e+01,  2.7311e+01,\n",
      "         -7.2375e+00, -1.0778e+01,  4.3776e-01, -2.5841e+00, -1.5039e+01,\n",
      "          7.3790e+00, -3.4593e+00, -1.4336e+01,  3.2283e+00, -1.5839e+01,\n",
      "          6.9807e+00, -1.6986e+01, -1.6937e+01, -2.3567e+01, -7.5183e+00,\n",
      "          2.5571e+00,  2.5247e-01, -1.4732e+00,  6.9981e+00,  1.5077e+01,\n",
      "         -8.5904e+00,  2.2573e+01, -1.0151e+01, -1.0546e+01,  2.7626e+01,\n",
      "         -3.5405e+01,  2.5024e-02,  7.3632e+00, -2.0875e+01, -1.2171e+01,\n",
      "         -8.2005e+00, -5.6964e+00,  2.6991e+01,  8.0040e+00, -2.4954e+01,\n",
      "          5.5852e+00, -5.9774e+00, -1.6032e+01, -1.2555e+01,  1.2618e+01,\n",
      "         -2.2407e+01, -2.0138e+01,  2.2398e+01, -3.0771e+01, -5.9408e+00,\n",
      "         -1.5607e+00, -2.3462e+01]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.embeddings(torch.tensor([0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "665631ae-ced6-46e4-98f6-fd8a159109f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(model.tokens, open(\"tokens.txt\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86d4e476-c226-4b98-8e6e-f09f02ca27d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.18080865603645\n",
      "177.001993166287\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "\n",
    "avg_res = 0\n",
    "avg_cont = 0\n",
    "t =0\n",
    "\n",
    "#file path to credit card csv file\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"mental_health.csv\")\n",
    "orig_dataset = pd.read_csv(file_path) #read csv file as pandas object\n",
    "orig_dataset =  orig_dataset.to_numpy()\n",
    "orig_dataset = orig_dataset.tolist()\n",
    "for example in orig_dataset:\n",
    "    acc =0 \n",
    "    for r_c in example:\n",
    "        if type(r_c) is float:\n",
    "            continue\n",
    "        if acc ==0: \n",
    "            avg_cont+=len(r_c.split())\n",
    "        else:\n",
    "            avg_res+=len(r_c.split())\n",
    "        acc+=1\n",
    "    t+=1\n",
    "print(avg_cont/t)\n",
    "print(avg_res/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d106792-36c6-4c23-8c79-439adabcf3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 23.1592,  30.9161,  10.1422,  ...,  -5.9408,  -1.5607, -23.4619],\n",
      "         [  7.1202, -11.3394, -21.1077,  ...,   4.3271,   8.4549,  -5.0601],\n",
      "         [ -4.8824,  -6.2988,  14.3777,  ...,  35.3968, -24.2729,  17.4410]],\n",
      "\n",
      "        [[ -4.5475,  -9.4168,   4.7978,  ...,  14.9789,  -9.0125, -16.8119],\n",
      "         [-10.9769,  -8.4734,   4.0464,  ...,  -5.4173,  27.6828, -16.7993],\n",
      "         [  9.3629, -16.2606,  18.8263,  ...,   7.8536,  -7.7093, -16.2574]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.embeddings(torch.tensor([[0, 1, 2], [3,4,5]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460f036-8936-4bb1-b31d-9a4028e234fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.load_state_dict(torch.load('transformer_model.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
