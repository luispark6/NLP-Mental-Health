{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "283a825a-8448-4e96-bb54-dfca2cd19aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e0fdea83-0b8e-4c0b-9507-b5ef6d1ae91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        #does not concacenate all input words, will give use tensors of each words output\n",
    "        #which are the embeddings of each word\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
    "        #now takes in all embeddings of each word stretched out\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #embeds will be flattened matrix\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        #rectified relu to learn embeddings\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "63bfd991-fc29-40b4-9c10-531edb2081aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22107\n",
      "0\n",
      "0.3550300666341622\n",
      "50499.68012473732\n",
      "1\n",
      "0.4933365837802698\n",
      "33208.18667971098\n",
      "2\n",
      "0.555501381439948\n",
      "26139.016567107887\n",
      "3\n",
      "0.6164472614984561\n",
      "20383.852552311975\n",
      "4\n",
      "0.6883633999674955\n",
      "15520.857827717562\n",
      "5\n",
      "0.7571103526734926\n",
      "11732.225720683346\n",
      "6\n",
      "0.8101738989111003\n",
      "8992.991721421531\n",
      "7\n",
      "0.8478790833739639\n",
      "7086.27178183269\n",
      "8\n",
      "0.8733138306517146\n",
      "5754.186520982173\n",
      "9\n",
      "0.8886721924264587\n",
      "4840.194456682844\n",
      "1.08317997791682\n"
     ]
    }
   ],
   "source": [
    "#file path to credit card csv file\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"mental_health.csv\")\n",
    "dataset = pd.read_csv(file_path) #read csv file as pandas object\n",
    "dataset = dataset.to_numpy()\n",
    "dataset = dataset[0:len(dataset)//30]\n",
    "\n",
    "\n",
    "test = dataset[(len(dataset)//30)+1:(len(dataset)//30)+ 10000]\n",
    "\n",
    "\n",
    "minFreq = {} #word must appear n times to be added to dictionary\n",
    "dictionary = {} #relevant words in the dicationary\n",
    "index = 2\n",
    "for example in range(len(dataset)):\n",
    "    for cont_response in range(2):\n",
    "        if type(dataset[example][cont_response]) == float:\n",
    "            continue\n",
    "        for word in dataset[example][cont_response].split():\n",
    "            if word not in minFreq:\n",
    "                minFreq[word]=1\n",
    "            else:\n",
    "                if minFreq[word]==3:\n",
    "                    dictionary[word] = index\n",
    "                    index+=1\n",
    "                minFreq[word]+=1\n",
    "CONTEXT_SIZE = 3 #look 3 words back to predict current word\n",
    "EMBEDDING_DIM = 252 #total embeddings for each word\n",
    "all_ngrams = [] #ngram setup -> [(['through', 'going', \"I'm\"], 'some')]\n",
    "for example in range(len(dataset)):\n",
    "    for cont_response in range(2):\n",
    "        if type(dataset[example][cont_response]) == float:\n",
    "            continue\n",
    "        cur_Sentence = dataset[example][cont_response].split()\n",
    "        ngrams = [\n",
    "            ([cur_Sentence[i - j - 1] for j in range(CONTEXT_SIZE)],cur_Sentence[i])\n",
    "            for i in range(CONTEXT_SIZE, len(cur_Sentence))\n",
    "            ]\n",
    "        for i in ngrams:\n",
    "            all_ngrams.append(i)\n",
    "\n",
    "            \n",
    "            \n",
    "test_grams = []\n",
    "for example in range(len(test)):\n",
    "    for cont_response in range(2):\n",
    "        if type(test[example][cont_response]) == float:\n",
    "            continue\n",
    "        cur_Sentence = test[example][cont_response].split()\n",
    "        sngrams = [\n",
    "            ([cur_Sentence[i - j - 1] for j in range(CONTEXT_SIZE)],cur_Sentence[i])\n",
    "            for i in range(CONTEXT_SIZE, len(cur_Sentence))\n",
    "            ]\n",
    "        for i in sngrams:\n",
    "            test_grams.append(i)\n",
    "            \n",
    "        \n",
    "            \n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(index, EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "print(len(all_ngrams))\n",
    "i = 0\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    total  = 0\n",
    "    correct = 0\n",
    "    print(epoch)\n",
    "    for context, target in all_ngrams:\n",
    "        i+=1\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        if context[0] not in dictionary or context[1] not in dictionary or context[2] not in dictionary:\n",
    "                continue\n",
    "        if target not in dictionary:\n",
    "                continue\n",
    "        \n",
    "        context_idxs = torch.tensor([dictionary[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "        \n",
    "        \n",
    "        predicted_classes = torch.max(log_probs, dim= 1)\n",
    "        predicted_index = int(predicted_classes[1]) \n",
    "        if predicted_index == dictionary[target]:\n",
    "            correct+=1\n",
    "            \n",
    "        \n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([dictionary[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        total+=1\n",
    "    print(correct/total)\n",
    "    print(total_loss)\n",
    "    losses.append(total_loss)\n",
    "# print(losses)  # The loss decreased every iteration over the training data!\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for context, target in test_grams:\n",
    "    i+=1\n",
    "    # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "    # into integer indices and wrap them in tensors)\n",
    "    if context[0] not in dictionary or context[1] not in dictionary or context[2] not in dictionary:\n",
    "            continue\n",
    "    if target not in dictionary:\n",
    "            continue\n",
    "\n",
    "    context_idxs = torch.tensor([dictionary[w] for w in context], dtype=torch.long)\n",
    "\n",
    "\n",
    "    # Step 3. Run the forward pass, getting log probabilities over next\n",
    "    # words\n",
    "    log_probs = model(context_idxs)\n",
    "    predicted_classes = torch.max(log_probs, dim= 1)\n",
    "    predicted_index = int(predicted_classes[1]) \n",
    "    if predicted_index == dictionary[target]:\n",
    "        correct+=1\n",
    "\n",
    "\n",
    "\n",
    "    # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "    # word wrapped in a tensor)\n",
    "    loss = loss_function(log_probs, torch.tensor([dictionary[target]], dtype=torch.long))\n",
    "    # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "    total_loss += loss.item()\n",
    "    total+=1\n",
    "print(correct/total)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bdd6f4-bc7f-44b2-8dd7-218d78a5437e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
