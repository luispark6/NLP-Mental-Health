{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283a825a-8448-4e96-bb54-dfca2cd19aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0fdea83-0b8e-4c0b-9507-b5ef6d1ae91e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        #does not pass all input words with eachother. Each word goes through independantly\n",
    "        #and the output are the embeddings of the word. We want this because we do not \n",
    "        #want to concacenate the embeddings to the output nodes.\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
    "        #now takes in all embeddings of each word stretched out\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.tokens = {}\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #embeds will be flattened matrix\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        #rectified relu to learn embeddings\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        #output is the log probablities of all vocabulary\n",
    "        return log_probs\n",
    "    def multiply_embedding_weights(self):\n",
    "        # Multiply the weights of the embedding layer by sqrt(embedding_dim)\n",
    "        self.embeddings.weight.data = self.embeddings.weight.data * (self.embedding_dim ** 0.5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63bfd991-fc29-40b4-9c10-531edb2081aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3512\n",
      "Train Dataset size: 1100\n",
      "Total Dictionary Size: 10,489\n",
      "Training Dictionary Size: 4928\n",
      "Epoch: 0\n",
      "Total_Loss: 60804.26291155815\n",
      "Epoch: 1\n",
      "Total_Loss: 46571.5169621706\n",
      "Epoch: 2\n",
      "Total_Loss: 41252.384687781334\n",
      "Epoch: 3\n",
      "Total_Loss: 36316.487325042486\n",
      "Epoch: 4\n",
      "Total_Loss: 30948.29644267261\n",
      "Epoch: 5\n",
      "Total_Loss: 24833.90383963287\n",
      "Epoch: 6\n",
      "Total_Loss: 18685.234041979536\n",
      "Epoch: 7\n",
      "Total_Loss: 13621.824801164097\n",
      "Epoch: 8\n",
      "Total_Loss: 9793.968410769536\n",
      "Epoch: 9\n",
      "Total_Loss: 6935.741658010666\n",
      "Epoch: 10\n",
      "Total_Loss: 4771.109002284051\n",
      "Epoch: 11\n",
      "Total_Loss: 3156.6321763049145\n",
      "Epoch: 12\n",
      "Total_Loss: 1982.497338891295\n",
      "Epoch: 13\n",
      "Total_Loss: 1187.9729907933788\n",
      "Epoch: 14\n",
      "Total_Loss: 679.8280201242029\n",
      "Epoch: 15\n",
      "Total_Loss: 385.161126025614\n",
      "Epoch: 16\n",
      "Total_Loss: 229.17182286976077\n",
      "Epoch: 17\n",
      "Total_Loss: 161.37878057601776\n",
      "Epoch: 18\n",
      "Total_Loss: 112.5939684582404\n",
      "Epoch: 19\n",
      "Total_Loss: 106.8165105948903\n",
      "Epoch: 20\n",
      "Total_Loss: 89.95412244207483\n",
      "Epoch: 21\n",
      "Total_Loss: 84.07676995928219\n",
      "Epoch: 22\n",
      "Total_Loss: 98.39077126342104\n",
      "Epoch: 23\n",
      "Total_Loss: 74.49512421116066\n",
      "Epoch: 24\n",
      "Total_Loss: 78.65696912404843\n"
     ]
    }
   ],
   "source": [
    "#file path to credit card csv file\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"mental_health.csv\")\n",
    "orig_dataset = pd.read_csv(file_path) #read csv file as pandas object\n",
    "orig_dataset = orig_dataset.to_numpy()\n",
    "print(\"Dataset size: \"+ str(len(orig_dataset)))\n",
    "dataset = orig_dataset[np.random.choice(orig_dataset.shape[0], size=1100, replace=True)]\n",
    "print(\"Train Dataset size: \"+ str(len(dataset)))\n",
    "minFreq = {} #word must appear n times to be added to dictionary\n",
    "dictionary = {} #relevant words in the dicationary\n",
    "index = 5\n",
    "for example in range(len(dataset)):\n",
    "    for cont_response in range(2):\n",
    "        if type(dataset[example][cont_response]) == float: #NaN values\n",
    "            continue\n",
    "        for word in dataset[example][cont_response].split():\n",
    "            if word not in minFreq:\n",
    "                minFreq[word]=1\n",
    "            else:\n",
    "                if minFreq[word]==3: #word needs to appear\n",
    "                    dictionary[word] = index\n",
    "                    index+=1\n",
    "                minFreq[word]+=1\n",
    "\n",
    "print( \"Total Dictionary Size: 10,489\")\n",
    "print(\"Training Dictionary Size: \" + str(index))\n",
    "\n",
    "CONTEXT_SIZE = 3 #look 3 words back to predict current word\n",
    "EMBEDDING_DIM = 252 #total embeddings for each word\n",
    "all_ngrams = [] #ngram setup -> [(['through', 'going', \"I'm\"], 'some')]\n",
    "for example in range(len(dataset)): \n",
    "    for cont_response in range(2): #context than response\n",
    "        if type(dataset[example][cont_response]) == float: #NaN values\n",
    "            continue\n",
    "        cur_Sentence = dataset[example][cont_response].split() #seperate by word\n",
    "        ngrams = [ #[(['through', 'going', \"I'm\"], 'some')]\n",
    "            ([cur_Sentence[i - j - 1] for j in range(CONTEXT_SIZE)],cur_Sentence[i])\n",
    "            for i in range(CONTEXT_SIZE, len(cur_Sentence))\n",
    "            ]\n",
    "        #append the grams to all_ngrams\n",
    "        for i in ngrams:\n",
    "            all_ngrams.append(i) \n",
    "loss_function = nn.NLLLoss() #loss layer\n",
    "model = NGramLanguageModeler(index, EMBEDDING_DIM, CONTEXT_SIZE) #intialize Ngram model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "model.tokens = dictionary\n",
    "for epoch in range(25):\n",
    "    total_loss = 0\n",
    "    print(\"Epoch: \"+ str(epoch))\n",
    "    maxFreq = 3 #max number of times a word can be trained\n",
    "    #dictionary to keep track of times word is trained. Will skip if words have been trained maxFreq times\n",
    "    maxFreqDict = {}\n",
    "    for context, target in all_ngrams:\n",
    "        #if unknown word, just don't train\n",
    "        if context[0] not in dictionary or context[1] not in dictionary or context[2] not in dictionary:\n",
    "                continue\n",
    "        if target not in dictionary:\n",
    "                continue\n",
    "        #add context words if not found in dict\n",
    "        if context[0] not in maxFreqDict:\n",
    "            maxFreqDict[context[0]] = 1\n",
    "        if context[1] not in maxFreqDict:\n",
    "            maxFreqDict[context[1]] = 1\n",
    "        #if both words have been trained equal to or more than maxFreq times, continue\n",
    "        #already has been trained enough\n",
    "        if maxFreqDict[context[0]] >= maxFreq and maxFreqDict[context[1]] >= maxFreq:\n",
    "            continue\n",
    "        #update how many times the context words have been trained\n",
    "        maxFreqDict[context[0]]+=1\n",
    "        maxFreqDict[context[1]]+=1\n",
    "            \n",
    "        #turn each word to an integer and wrapped in tensor so pass as an input to the model\n",
    "        context_idxs = torch.tensor([dictionary[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        #zero out gradients cause it accumulates\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "        #apply the loss function to the log probabilties with the correct target word\n",
    "        loss = loss_function(log_probs, torch.tensor([dictionary[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Total_Loss: {total_loss}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87bdd6f4-bc7f-44b2-8dd7-218d78a5437e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.multiply_embedding_weights()\n",
    "torch.save(model.state_dict(), \"embedding_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3d7463e-6ccc-4042-ae90-636c10dcaec6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-27.3289,   0.5755,   7.6977,  -1.7063,  11.6704, -16.0344, -13.8994,\n",
      "          -5.5939,  23.0890,  -4.3854,  -8.6859,  10.1936,  24.3662, -28.8604,\n",
      "         -10.1478, -15.4639, -33.8165, -29.0961, -15.7963,   4.3913, -10.6751,\n",
      "          -5.4325,  -1.3452, -46.0790,  40.6227,   8.9821,  -8.2570,  -4.0520,\n",
      "          -2.2921, -19.6806,  20.7959,  17.1168,  13.2371,  -6.3065, -14.8282,\n",
      "          -6.2137,   1.8696,  25.3100,   1.8748,  -5.2926,  11.0353,  10.8461,\n",
      "          -9.0835,  -9.5109,   4.8956,  11.1274, -32.6840, -25.0068,   3.6075,\n",
      "          -4.2316, -31.4773, -15.6223,  13.5155,  -9.6214, -46.1658,  -3.8901,\n",
      "          25.2988,  -2.0704,  13.7544, -32.0602,  -5.4711,  -6.2360,  -0.6747,\n",
      "         -14.0538,   0.8529,  12.0244,  34.4057,  -2.3645,  11.3389,   3.8502,\n",
      "          -0.3880,  14.1577,  15.6571,   1.5358,  -1.1155,  -5.0325,   2.5739,\n",
      "          -3.1278,  14.8901,  15.7941,  19.9152,   4.4657,  27.4171,  12.8808,\n",
      "           5.6889,   5.4184, -10.4300,  10.7177,   9.2347, -19.8214,  -5.0721,\n",
      "          12.1199,   7.0368,   3.2598,  18.9480,   1.3039,  13.6849, -12.7381,\n",
      "          -9.9445,   5.6234, -18.6196, -11.7445,   8.0713, -16.1013,   6.0403,\n",
      "          14.2638,  23.0508,  13.2208,  -8.1905,  -0.8476,  15.6956, -11.8326,\n",
      "         -24.3290,   8.5115,   4.9052, -24.1249,   6.9230,  20.4213,   4.1819,\n",
      "          29.9562,   3.6696,  -0.1411,  -0.1007,  22.7975, -20.2660, -14.1050,\n",
      "         -19.0713, -22.0953, -19.5927,   6.2763, -30.1219, -19.2937,  44.7706,\n",
      "          25.6000,  22.1675, -29.2930,  -2.7346,  -3.4416,  -5.1301,  -1.5462,\n",
      "         -33.7653, -18.2661,  19.9216, -36.7882,  -7.0953,   3.1845,  -4.1410,\n",
      "          -2.3792,  13.6013,  13.3258,   4.5690,  -9.3586,   7.8694,   2.7397,\n",
      "          -7.6597,  24.7637,  -5.1077,   1.2689,  27.9641,  19.2072,  -7.8156,\n",
      "         -27.0865, -15.9877,  -0.7460, -13.0699,  -3.3085, -13.2278, -14.1908,\n",
      "          -2.4909,  26.7663, -10.3620,  -2.2131, -14.9372,  -0.2627,  10.5170,\n",
      "          16.1916, -10.2935,  33.0164,  13.9333,   4.0142, -11.1900,  18.8919,\n",
      "           6.1980,   8.2655,  -9.6167,   4.3411, -16.8982,  -3.5859,   7.0150,\n",
      "           0.1140,   2.2152,   7.9691,  -5.5870,   6.4571,  13.0311,  -1.8810,\n",
      "          -8.7489,  28.9119,  12.5364, -11.7786, -12.6795, -17.4474,  -1.8753,\n",
      "           0.3644, -12.0099,  10.5288,  -6.7368,  16.8278, -24.1257, -20.6118,\n",
      "         -24.8713,  -2.3118,  -4.5003,  27.2479,  20.0544, -14.4488,  32.0507,\n",
      "          -4.6161, -12.9276,  -1.7654,   6.6837,   2.2645,  12.6414,  -5.8620,\n",
      "           4.2139,  16.9244,  20.0207, -23.8099, -19.6057,  10.9208,   6.2321,\n",
      "          -6.2978,  25.0758,  12.9984,  -3.3280,  -0.9529,   0.3103,   0.6286,\n",
      "          18.0383,  15.8558, -14.1447, -31.4643,  -5.7773,  -6.9185,  -7.9769,\n",
      "         -18.1364,   2.8495,   0.9376,  17.4529,  17.1137,  29.6456,  17.2282]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.embeddings(torch.tensor([0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "665631ae-ced6-46e4-98f6-fd8a159109f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(model.tokens, open(\"tokens.txt\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86d4e476-c226-4b98-8e6e-f09f02ca27d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.18080865603645\n",
      "177.001993166287\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "\n",
    "avg_res = 0\n",
    "avg_cont = 0\n",
    "t =0\n",
    "\n",
    "#file path to credit card csv file\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"mental_health.csv\")\n",
    "orig_dataset = pd.read_csv(file_path) #read csv file as pandas object\n",
    "orig_dataset =  orig_dataset.to_numpy()\n",
    "orig_dataset = orig_dataset.tolist()\n",
    "for example in orig_dataset:\n",
    "    acc =0 \n",
    "    for r_c in example:\n",
    "        if type(r_c) is float:\n",
    "            continue\n",
    "        if acc ==0: \n",
    "            avg_cont+=len(r_c.split())\n",
    "        else:\n",
    "            avg_res+=len(r_c.split())\n",
    "        acc+=1\n",
    "    t+=1\n",
    "print(avg_cont/t)\n",
    "print(avg_res/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d106792-36c6-4c23-8c79-439adabcf3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.7329e+01,  5.7545e-01,  7.6977e+00, -1.7063e+00,  1.1670e+01,\n",
      "         -1.6034e+01, -1.3899e+01, -5.5939e+00,  2.3089e+01, -4.3854e+00,\n",
      "         -8.6859e+00,  1.0194e+01,  2.4366e+01, -2.8860e+01, -1.0148e+01,\n",
      "         -1.5464e+01, -3.3817e+01, -2.9096e+01, -1.5796e+01,  4.3913e+00,\n",
      "         -1.0675e+01, -5.4325e+00, -1.3452e+00, -4.6079e+01,  4.0623e+01,\n",
      "          8.9821e+00, -8.2570e+00, -4.0520e+00, -2.2921e+00, -1.9681e+01,\n",
      "          2.0796e+01,  1.7117e+01,  1.3237e+01, -6.3065e+00, -1.4828e+01,\n",
      "         -6.2137e+00,  1.8696e+00,  2.5310e+01,  1.8748e+00, -5.2926e+00,\n",
      "          1.1035e+01,  1.0846e+01, -9.0835e+00, -9.5109e+00,  4.8956e+00,\n",
      "          1.1127e+01, -3.2684e+01, -2.5007e+01,  3.6075e+00, -4.2316e+00,\n",
      "         -3.1477e+01, -1.5622e+01,  1.3515e+01, -9.6214e+00, -4.6166e+01,\n",
      "         -3.8901e+00,  2.5299e+01, -2.0704e+00,  1.3754e+01, -3.2060e+01,\n",
      "         -5.4711e+00, -6.2360e+00, -6.7468e-01, -1.4054e+01,  8.5287e-01,\n",
      "          1.2024e+01,  3.4406e+01, -2.3645e+00,  1.1339e+01,  3.8502e+00,\n",
      "         -3.8799e-01,  1.4158e+01,  1.5657e+01,  1.5358e+00, -1.1155e+00,\n",
      "         -5.0325e+00,  2.5739e+00, -3.1278e+00,  1.4890e+01,  1.5794e+01,\n",
      "          1.9915e+01,  4.4657e+00,  2.7417e+01,  1.2881e+01,  5.6889e+00,\n",
      "          5.4184e+00, -1.0430e+01,  1.0718e+01,  9.2347e+00, -1.9821e+01,\n",
      "         -5.0721e+00,  1.2120e+01,  7.0368e+00,  3.2598e+00,  1.8948e+01,\n",
      "          1.3039e+00,  1.3685e+01, -1.2738e+01, -9.9445e+00,  5.6234e+00,\n",
      "         -1.8620e+01, -1.1745e+01,  8.0713e+00, -1.6101e+01,  6.0403e+00,\n",
      "          1.4264e+01,  2.3051e+01,  1.3221e+01, -8.1905e+00, -8.4760e-01,\n",
      "          1.5696e+01, -1.1833e+01, -2.4329e+01,  8.5115e+00,  4.9052e+00,\n",
      "         -2.4125e+01,  6.9230e+00,  2.0421e+01,  4.1819e+00,  2.9956e+01,\n",
      "          3.6696e+00, -1.4113e-01, -1.0066e-01,  2.2797e+01, -2.0266e+01,\n",
      "         -1.4105e+01, -1.9071e+01, -2.2095e+01, -1.9593e+01,  6.2763e+00,\n",
      "         -3.0122e+01, -1.9294e+01,  4.4771e+01,  2.5600e+01,  2.2167e+01,\n",
      "         -2.9293e+01, -2.7346e+00, -3.4416e+00, -5.1301e+00, -1.5462e+00,\n",
      "         -3.3765e+01, -1.8266e+01,  1.9922e+01, -3.6788e+01, -7.0953e+00,\n",
      "          3.1845e+00, -4.1410e+00, -2.3792e+00,  1.3601e+01,  1.3326e+01,\n",
      "          4.5690e+00, -9.3586e+00,  7.8694e+00,  2.7397e+00, -7.6597e+00,\n",
      "          2.4764e+01, -5.1077e+00,  1.2689e+00,  2.7964e+01,  1.9207e+01,\n",
      "         -7.8156e+00, -2.7087e+01, -1.5988e+01, -7.4602e-01, -1.3070e+01,\n",
      "         -3.3085e+00, -1.3228e+01, -1.4191e+01, -2.4909e+00,  2.6766e+01,\n",
      "         -1.0362e+01, -2.2131e+00, -1.4937e+01, -2.6270e-01,  1.0517e+01,\n",
      "          1.6192e+01, -1.0293e+01,  3.3016e+01,  1.3933e+01,  4.0142e+00,\n",
      "         -1.1190e+01,  1.8892e+01,  6.1980e+00,  8.2655e+00, -9.6167e+00,\n",
      "          4.3411e+00, -1.6898e+01, -3.5859e+00,  7.0150e+00,  1.1398e-01,\n",
      "          2.2152e+00,  7.9691e+00, -5.5870e+00,  6.4571e+00,  1.3031e+01,\n",
      "         -1.8810e+00, -8.7489e+00,  2.8912e+01,  1.2536e+01, -1.1779e+01,\n",
      "         -1.2679e+01, -1.7447e+01, -1.8753e+00,  3.6435e-01, -1.2010e+01,\n",
      "          1.0529e+01, -6.7368e+00,  1.6828e+01, -2.4126e+01, -2.0612e+01,\n",
      "         -2.4871e+01, -2.3118e+00, -4.5003e+00,  2.7248e+01,  2.0054e+01,\n",
      "         -1.4449e+01,  3.2051e+01, -4.6161e+00, -1.2928e+01, -1.7654e+00,\n",
      "          6.6837e+00,  2.2645e+00,  1.2641e+01, -5.8620e+00,  4.2139e+00,\n",
      "          1.6924e+01,  2.0021e+01, -2.3810e+01, -1.9606e+01,  1.0921e+01,\n",
      "          6.2321e+00, -6.2978e+00,  2.5076e+01,  1.2998e+01, -3.3280e+00,\n",
      "         -9.5289e-01,  3.1027e-01,  6.2859e-01,  1.8038e+01,  1.5856e+01,\n",
      "         -1.4145e+01, -3.1464e+01, -5.7773e+00, -6.9185e+00, -7.9769e+00,\n",
      "         -1.8136e+01,  2.8495e+00,  9.3760e-01,  1.7453e+01,  1.7114e+01,\n",
      "          2.9646e+01,  1.7228e+01],\n",
      "        [ 2.3521e+01, -1.1825e+01,  2.9408e+01, -6.5674e+00,  1.6016e+01,\n",
      "         -1.2269e+01, -1.9494e+01,  2.0546e+01,  6.0700e+00, -5.0704e-01,\n",
      "         -1.1822e+01,  1.5013e+01, -2.2216e+01,  1.5354e+01, -2.4018e+01,\n",
      "         -1.0514e+01,  8.3878e+00,  1.2680e+01, -6.5763e+00,  7.9870e+00,\n",
      "          6.3967e+00,  6.8202e+00,  3.9890e-01,  4.0246e+00, -1.6656e+01,\n",
      "         -6.7600e+00, -1.3967e+01,  7.5269e+00,  1.1612e+01,  2.3700e+01,\n",
      "          1.2251e+01, -6.7433e+00,  2.3514e+01, -8.3521e+00,  6.3702e+00,\n",
      "          2.3389e+01,  1.6742e+01, -4.4362e+00, -7.0383e+00,  2.3447e+01,\n",
      "          1.8664e+01,  1.6296e+00,  6.9619e+00,  2.1945e+01,  2.8001e+00,\n",
      "         -8.3870e+00,  4.3871e+00,  1.1215e+01, -1.0631e+01,  1.7658e+01,\n",
      "          1.6386e+00,  2.7504e+00, -5.9400e+00, -2.7057e+01,  1.5560e+01,\n",
      "         -1.3183e+01,  8.0457e+00, -1.4939e+01,  8.2391e+00, -8.3604e+00,\n",
      "         -1.2326e+00,  2.7128e+01, -2.6382e+00, -4.7390e+00,  2.3246e+01,\n",
      "          1.1399e+01, -8.2686e+00,  2.1941e+01,  1.3033e+01, -1.3906e+01,\n",
      "          6.5323e+00, -1.6110e+01,  6.7798e+00,  2.7022e+01, -2.1353e+00,\n",
      "          1.6211e+00, -2.3291e+00, -1.5674e+01,  1.5937e+01, -9.1217e+00,\n",
      "         -5.6627e-01,  1.3777e+01,  7.0531e-01,  7.9231e+00, -1.3229e+01,\n",
      "         -1.0279e+01, -1.0407e+00,  3.5822e+01, -6.1769e+00, -1.1886e+01,\n",
      "          1.0464e+00,  2.9373e+00, -2.1672e+01, -6.8586e+00, -4.2633e+01,\n",
      "         -2.5664e+01,  1.0245e+01, -4.8070e+00,  2.0148e+01, -4.6636e+00,\n",
      "          2.5260e+00, -1.1765e+01, -1.5515e+01, -5.3929e+00, -9.2543e+00,\n",
      "          8.4976e-01, -2.3370e+01,  1.1392e+01, -4.8459e+00,  1.9451e+01,\n",
      "          5.4457e+00,  2.0212e+01,  2.2361e+00,  5.9017e+00, -1.5926e+01,\n",
      "         -2.5933e+01, -1.0232e+01, -1.7682e+00, -1.8900e+01,  1.8252e+01,\n",
      "          1.2468e+01,  6.5102e+00,  2.3407e+00, -3.8063e+00, -2.8846e+01,\n",
      "         -8.0793e+00,  1.1664e+01,  1.2402e+01, -1.1688e+01,  3.8981e+00,\n",
      "          2.2159e+01, -4.3904e+00,  1.9600e+01, -3.5564e+01, -1.5689e+01,\n",
      "         -1.7507e+00,  5.7134e+00, -3.0702e+01,  1.9315e+01,  2.2019e+01,\n",
      "         -2.0356e+01, -2.2447e+00, -1.2707e+01, -7.3906e+00, -5.2822e+00,\n",
      "          1.2013e+01,  1.0630e+01,  3.1245e+01, -1.0224e+01, -1.8572e+01,\n",
      "         -1.2347e+01,  4.6587e+00, -2.0851e+00, -1.6348e+01,  1.3927e+00,\n",
      "          1.2147e+01, -1.9389e+00,  1.3989e+01, -5.1197e+00,  1.3707e+01,\n",
      "          2.7349e+01, -7.6128e+00, -1.1308e+01, -2.7435e+00,  3.7461e+00,\n",
      "         -1.3815e+01, -6.3149e+00,  9.0902e+00,  8.0304e+00,  3.9708e+00,\n",
      "          7.5107e-01, -8.5062e+00, -7.2346e+00,  3.6546e+00,  1.5606e+01,\n",
      "          8.5323e+00, -1.4364e+01, -3.2448e+00, -1.5515e+01, -1.3633e+01,\n",
      "          2.0057e+01, -1.9844e+00, -6.4934e+00,  1.6694e+00,  3.0139e+01,\n",
      "          6.7392e-01,  9.8452e+00, -7.1759e-01,  1.5038e+01,  4.1512e+00,\n",
      "         -7.5812e+00, -7.2622e+00,  1.9929e+01, -1.0033e+01, -2.8105e+01,\n",
      "         -4.9558e+01, -1.9603e+01, -6.3326e+00,  2.1340e+01,  8.3720e+00,\n",
      "         -1.3813e+01,  2.9810e+01,  9.6951e+00,  1.5703e+01, -1.0389e+01,\n",
      "         -8.6722e+00,  1.6119e+01, -2.0918e+01,  1.0232e+01,  2.2945e+01,\n",
      "         -2.3312e+00,  1.1405e+00,  3.3796e+01,  2.2055e+00, -2.6710e+01,\n",
      "          8.8088e+00,  1.1640e+00,  1.1667e+01, -2.1251e+01, -1.0980e+01,\n",
      "          1.4894e+01, -1.2271e+01, -6.6579e+00, -2.1879e+01,  6.3413e-01,\n",
      "         -3.5287e+01,  1.1074e+00,  4.1401e+00,  1.2325e+01, -3.3622e+01,\n",
      "          3.5193e+01, -7.4215e+00,  9.7286e+00, -1.3401e+01,  1.2862e+01,\n",
      "          1.1998e+01, -1.8657e+01, -2.3665e+01,  5.6043e+00,  9.5052e+00,\n",
      "          8.4435e+00,  2.0821e+01,  9.1933e+00,  1.8620e+01, -9.5863e+00,\n",
      "         -7.2822e+00, -3.1338e+01,  1.6923e+01, -6.0759e+00,  1.7719e+01,\n",
      "          3.0190e+01, -5.7672e+00],\n",
      "        [ 2.4261e+01, -7.0546e+00,  3.5673e+01,  1.5743e+01,  1.8863e+01,\n",
      "          3.3567e+00,  1.7587e+00, -1.6243e+01, -2.9899e+01, -4.6833e+00,\n",
      "         -8.6071e+00,  1.6511e+01, -7.6838e+00, -2.5432e+01,  1.2315e+01,\n",
      "         -6.7142e+00, -1.0987e+01,  2.9873e+00,  1.1238e+01, -2.1138e+01,\n",
      "         -5.8674e+00, -1.2617e+01, -2.5274e+01, -1.0199e+01, -5.8483e+00,\n",
      "          6.4940e+00, -8.2823e-02, -3.9356e-01, -7.5988e+00, -4.8067e+01,\n",
      "          1.8013e+01,  3.7982e+00, -2.0513e+01,  5.1144e+00,  1.1320e+01,\n",
      "          6.6274e+00, -8.9288e+00,  2.5419e+01, -2.6335e-01, -1.5187e+01,\n",
      "         -1.4245e+00, -3.2178e+00,  1.8439e+01, -6.0591e+00, -9.0768e+00,\n",
      "          3.7847e-01, -1.6747e+01, -3.2153e+00, -1.5014e+01,  1.3362e+01,\n",
      "         -3.3295e+01, -1.2249e+01,  7.8260e+00, -1.7024e+01,  7.4101e+00,\n",
      "          1.5295e+01, -1.4078e+01,  1.6173e+01,  9.5520e+00, -1.1703e+01,\n",
      "         -2.1427e+01, -9.3010e+00, -1.9338e+01,  2.4203e+01,  7.3645e+00,\n",
      "         -1.2447e+01, -3.6194e+00, -1.8198e+01,  2.2390e+01, -4.4566e+00,\n",
      "          1.2867e+01,  4.1660e+00, -3.3405e+00,  9.3277e+00, -3.0263e+01,\n",
      "         -2.5513e+01,  9.1806e+00, -3.7800e-01,  2.5524e+01, -1.5257e+01,\n",
      "         -2.8581e+00, -1.3124e+00,  2.2662e+01, -1.9688e+01,  5.3530e+00,\n",
      "         -2.2596e+01,  3.8829e+00, -5.2881e+00, -4.1863e+00,  9.7299e-01,\n",
      "          3.1075e+00, -1.8691e+01, -1.9312e+01, -7.7738e+00,  1.4014e+01,\n",
      "         -3.0346e+01,  9.0960e+00,  1.0343e+01, -1.2063e+01,  2.0791e-01,\n",
      "          2.1450e+01,  6.0845e+00,  1.3378e+01,  1.2860e+01, -1.2853e+01,\n",
      "          1.3436e+01,  2.0445e+00,  1.8190e+01,  7.6363e+00, -1.8559e+01,\n",
      "         -1.3644e+01, -1.4567e+01,  7.2566e-01, -2.9856e+01,  3.1953e+00,\n",
      "          1.2671e+01, -8.7718e+00, -1.0008e+01,  8.2478e+00, -1.9309e+01,\n",
      "         -4.6677e+00, -1.3554e+01, -1.0544e+01, -6.0774e+00, -3.5139e+01,\n",
      "         -2.6462e+00,  5.9064e+00,  3.8454e-01, -6.2642e-01,  1.0780e+01,\n",
      "         -2.1638e+01, -9.6183e+00,  9.2373e+00, -9.2124e+00,  9.0704e+00,\n",
      "         -8.8268e+00, -1.0809e+01,  4.0462e+00, -4.5681e+00,  1.4846e+01,\n",
      "         -1.0057e+01, -4.9559e+00, -4.1465e+00,  2.3981e+00,  2.3848e+00,\n",
      "         -2.9711e+01,  1.7796e+01, -4.8939e+00, -1.8726e+01,  9.8642e+00,\n",
      "          1.5437e+01, -4.6343e+00, -2.8556e+01, -6.2460e+00,  4.1222e+01,\n",
      "         -3.0164e+00, -1.6392e+01, -1.7483e+01,  2.1451e+00, -2.0388e+01,\n",
      "         -2.5597e+01, -1.3436e+01,  5.2159e+00,  1.5399e+01,  2.2819e+01,\n",
      "          7.5470e+00, -4.0867e+00,  8.7216e+00,  9.5396e+00,  1.3710e+01,\n",
      "         -2.4327e+00, -1.3646e+01,  2.1932e+00, -3.6882e+01,  4.5262e+00,\n",
      "         -1.2528e+00,  1.4001e+01, -2.2168e+01,  5.0754e+00, -2.4807e+01,\n",
      "         -1.3491e+01,  1.1756e+01, -1.5717e+01,  3.0482e-01,  2.0279e+01,\n",
      "         -1.4314e+01, -3.3334e+00, -1.7229e+01, -1.4322e+01,  2.7852e+01,\n",
      "         -7.0061e+00,  2.9512e+00, -1.8566e+01,  1.8062e+01,  2.3955e+01,\n",
      "          3.1308e-02, -4.3773e+00,  3.2782e+01, -3.1236e+00,  1.1193e+01,\n",
      "          1.0336e+01,  1.8754e+01,  1.2637e+01, -2.1599e+01,  1.3197e+01,\n",
      "          1.9548e+01, -8.9187e+00,  5.1731e+00,  3.4841e+01, -1.7790e+01,\n",
      "          5.2077e+00,  1.1991e+00, -3.9348e+00, -2.5528e+01, -2.2532e+01,\n",
      "          1.8022e+01,  1.9665e+01, -4.1680e+00, -5.3312e+00,  5.2092e-02,\n",
      "          4.5677e+00, -1.7362e+01, -6.0253e+00,  2.3756e+01, -7.3435e+00,\n",
      "         -1.3806e+01, -1.1366e+01, -1.5557e+01, -3.9051e+00,  1.5927e+01,\n",
      "         -3.4332e+00, -1.4032e+01,  9.8010e+00,  2.5082e+01,  2.3543e+01,\n",
      "         -1.0627e+01, -9.8121e+00,  7.5491e+00,  1.0224e+01, -2.2552e+00,\n",
      "         -2.5231e+01,  1.5046e+01, -3.3639e+01,  9.5187e-01,  1.0202e+01,\n",
      "         -5.0144e-01,  1.1180e+01,  1.0288e+01,  2.9558e+01,  6.1224e-01,\n",
      "          2.0705e+01, -1.0988e+01]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.embeddings(torch.tensor([[0, 1, 2], [3,4,5]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460f036-8936-4bb1-b31d-9a4028e234fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
