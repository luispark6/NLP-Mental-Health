{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca86dc3-6a8c-46f6-a249-4a7fdcb4acee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 23.1592,  30.9161,  10.1422,  ...,  -5.9408,  -1.5607, -23.4619],\n",
      "        [  7.1202, -11.3394, -21.1077,  ...,   4.3271,   8.4549,  -5.0601],\n",
      "        [ -4.8824,  -6.2988,  14.3777,  ...,  35.3968, -24.2729,  17.4410],\n",
      "        ...,\n",
      "        [-17.2369,  -3.3502,   1.3883,  ...,   8.1477,  15.2713,  18.8054],\n",
      "        [-13.6703,   9.9790,  14.0219,  ...,  -6.9127,   4.1610, -37.5172],\n",
      "        [-20.6586,  14.1126, -20.4766,  ...,   9.8981,  -2.3947,  18.8373]])\n",
      "torch.Size([64, 200, 252])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import copy\n",
    "from torch import nn, Tensor\n",
    "VOCAB_SIZE = 6764\n",
    "D_MODEL = 252\n",
    "PADDING_TOKEN = 0\n",
    "UNK_TOKEN = 1\n",
    "SOS_TOKEN = 2\n",
    "EOS_TOKEN = 3\n",
    "MAX_LENGTH = 200\n",
    "tokens = json.load(open(\"tokens.txt\"))\n",
    "model = torch.load(\"embedding_model\")\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"mental_health.csv\")\n",
    "orig_dataset = pd.read_csv(file_path)\n",
    "orig_dataset = orig_dataset.to_numpy()\n",
    "unfiltered_trainingSet =  orig_dataset[np.random.choice(orig_dataset.shape[0], 100, replace=True)] #extract training set\n",
    "trainingSet = []\n",
    "for example in unfiltered_trainingSet:\n",
    "    if type(example[0]) is not float and type(example[1]) is not float:\n",
    "        if len(example[0].split())<200 and len(example[1].split())<200:\n",
    "            trainingSet.append(example)\n",
    "\n",
    "            \n",
    "contextSet = [trainingSet[i][0] for i in range(len(trainingSet))] \n",
    "responseSet = [trainingSet[i][1] for i in range(len(trainingSet))]\n",
    "#tokenizing the context and response set, also 1 is special token for unknown word\n",
    "contextSet_tokenized = [[tokens[word] if word in tokens else UNK_TOKEN for word in example.split()] \n",
    "                         for example in contextSet]\n",
    "\n",
    "\n",
    "responseSet_tokenized = [[tokens[word] if word in tokens else UNK_TOKEN for word in example.split()] \n",
    "                         for example in responseSet ]\n",
    "\n",
    "label =copy.deepcopy(responseSet_tokenized)\n",
    "#set up special tokens\n",
    "for i in range(len(contextSet_tokenized)):\n",
    "    while len(contextSet_tokenized[i])!= MAX_LENGTH:\n",
    "        contextSet_tokenized[i].append(PADDING_TOKEN)\n",
    "for i in range(len(responseSet_tokenized)):\n",
    "    responseSet_tokenized[i].insert(0, SOS_TOKEN)\n",
    "    label[i].append(EOS_TOKEN)\n",
    "for i in range(len(responseSet_tokenized)):\n",
    "    while len(responseSet_tokenized[i]) != MAX_LENGTH:\n",
    "        responseSet_tokenized[i].append(PADDING_TOKEN)\n",
    "        label[i].append(PADDING_TOKEN)\n",
    "\n",
    "    \n",
    "#convert token to input embedding for context and response set filled with padding\n",
    "#tokens if end of sentence. 0 is special token for padding \n",
    "contextSet_embedding = []        \n",
    "for context in contextSet_tokenized:\n",
    "    contextEmbedding = []\n",
    "    for i in range(MAX_LENGTH):\n",
    "        if i>= len(context):\n",
    "            contextEmbedding.append(model[\"embeddings.weight\"][1])\n",
    "            continue\n",
    "        contextEmbedding.append(model[\"embeddings.weight\"][context[i]])\n",
    "    contextEmbedding = torch.stack(contextEmbedding)\n",
    "    contextSet_embedding.append(contextEmbedding[:])\n",
    "\n",
    "contextSet_embedding = torch.stack(contextSet_embedding)\n",
    "responseSet_embedding = []        \n",
    "for response in responseSet_tokenized:\n",
    "    responseEmbedding = []\n",
    "    for i in range(MAX_LENGTH):\n",
    "        responseEmbedding.append(model[\"embeddings.weight\"][response[i]])\n",
    "    responseEmbedding = torch.stack(responseEmbedding)\n",
    "    responseSet_embedding.append(responseEmbedding[:])\n",
    "\n",
    "contextSet_tokenized = torch.tensor(contextSet_tokenized)\n",
    "responseSet_tokenized = torch.tensor(responseSet_tokenized)\n",
    "\n",
    "responseSet_embedding = torch.stack(responseSet_embedding)\n",
    "print(model['embeddings.weight'])\n",
    "pretrained_weights = model['embeddings.weight']\n",
    "print(responseSet_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba420137-3b54-46c9-83a5-84b855b7a7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int, custom_weights: torch.Tensor = None) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        if custom_weights is not None:\n",
    "            self.custom_embedding = nn.Parameter(custom_weights)\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        if self.custom_embedding is not None:\n",
    "            return F.embedding(x, self.custom_embedding) * math.sqrt(self.d_model)\n",
    "        else:\n",
    "            # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "            return self.embedding(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model = D_MODEL, seq_len = MAX_LENGTH, dropout = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "764d36e8-0be8-45bf-849a-18fca3070ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float)->None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # linear layer for queue\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # liner layer for key\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # linear layer for \n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "                 \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        #d_k is number of embeddings per word for each head\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        # [[[[4,3],[2,1]],[[2, 5], [3,2]]]] * [[[[4,3][2,5]],[[2,1], [3,2]]]] = \n",
    "        #[[[[0.7, 0.3], [0.1, 0.9]], [0.8, 0.2], [0.4, 0.6]]]\n",
    "        #basically saying the first word of the first half of the embedding(for head 1) is related to\n",
    "        #to its own word by 0.7 and related to the first half embedding of the second word by 0.3\n",
    "        #also saying the second word of the first half of the embedding(for head 1) is related to\n",
    "        #to its own word by 0.9 and related to the first half embedding of the first word by 0.1\n",
    "        #essentially a attention matrix for each part of the embedding for the head\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        #if mask is true, then make all appropriate masked attentions scores to very low value\n",
    "        #so that the soft max will ignore their scores\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        #[[[4,3,2,1] , [2,5,3,2]]]\n",
    "        #3rd dimension represents word embeddings of a word\n",
    "        #2nd dimension represent each word embedding in an example\n",
    "        #1st dimension represents each example containing each word embedding\n",
    "        # * Transform the dimensions using .view *\n",
    "        # [[[[4,3],[2,1]],[[2, 5], [3,2]]]]\n",
    "        #4th dimenension represents the size of the split embeddings for each multi-head attention\n",
    "        #3rd dimension represents the number of heads. Note that head times the size of split embeddings equals original embedding dimension\n",
    "        #2nd dimension represents sequence length\n",
    "        #1st dimension represents example lenght\n",
    "        # * Transponse dimensions 1,2 *\n",
    "        # [[[[4,3][2,5]],[[2,1], [3,2]]]]\n",
    "        # 4th dimension represents the size of the split embeddings for each multi-head attention\n",
    "        # 3rd dimension represents the length of the sequence. Contains each word with its associated head\n",
    "        # 2nd dimension represents the number of heads\n",
    "        # 1st dimension is the number of examples\n",
    "        #[[[[word 1 with first half embeddings][word 2 with first half embedding]],[[word 1 with second half], [word 2 with second half]]]]        \n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        #pass combined heads to last linear layer\n",
    "        return self.w_o(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc1610c-d6f6-4ef8-975d-223a4c8ac77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        #the output of the self.linear1 layer is the input of a relu layer\n",
    "        #the output of that is the input of linear layer 2\n",
    "     \n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb762885-8309-40e1-978b-62b8cdacbe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the data for stabilized training\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53b4e705-6736-4c6b-ba6b-6bcf42c4f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "    def forward(self, x, sublayer):\n",
    "        # in the paper, you would typically add x to the sublayer and then normalize the output\n",
    "        #of the sublayer, but in this case, we normalize before passing it in to the sublayer\n",
    "        #the .norm part is the normalizing part in ADD and NORM, and the addition of x is the \n",
    "        #ADD part of ADD and NORM\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89ad844d-9fca-4fbf-9343-e1900f44d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        #encoder block contains 2 residual connections\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        #Passes self attention block to residuals. Residuals will perform the attention block\n",
    "        #and also perform the ADD and NORM\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        #passes the output of the previous residual_connection layer('x'). Then, passes in a feed_forward_block\n",
    "        #Residual connection will perform the feed forward and ADD and NORM\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a51e5746-97a1-468f-83c9-60206377b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        #layers of encoder blocks\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        #perform a forward method on every encoder block. The output of each encoder\n",
    "        #becomes the input of the new encoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        #normalize the final output\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e87e855-5461-4bf9-be13-049368037b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "    \n",
    "    #src mask are for the encoder output. Do not want to attend to padding tokens\n",
    "    #tgt mask are for decoder input. Doesn't let you look into the future\n",
    "    #typically these just always be true I think\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8345ccd-9a72-41f3-a9f7-7c491fb67580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0b49481-6256-440b-a008-d267f62b4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear layer where output represents all words. We are going to soft max this \n",
    "#in the future. \n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c306bd0f-0a09-4729-ab51-c5d139ce52bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, src_pos: PositionalEncoding, projection_layer: ProjectionLayer, src_pad_indx = 0, device = \"cpu\") -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.projection_layer = projection_layer\n",
    "        self.src_pad_idx = src_pad_indx\n",
    "        self._pad_idx = src_pad_indx\n",
    "        self.device = device\n",
    "    \n",
    "    def make_src_mask(self, src_tokens):\n",
    "        src_mask = (src_tokens != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "    \n",
    "    def make_trg_mask(self, trg_tokens):\n",
    "        N, trg_len = trg_tokens.shape\n",
    "        trg_pad_mask = (trg_tokens != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "        trg_pad_mask= trg_pad_mask.to(self.device)\n",
    "        trg_sub_mask= trg_sub_mask.to(self.device)\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        # print(trg_mask)\n",
    "        \n",
    "        # N, trg_len = trg_tokens.shape\n",
    "        # trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "        #     N, 1, trg_len, trg_len\n",
    "        # )\n",
    "\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.src_embed(tgt)\n",
    "        tgt = self.src_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "897029fa-3e85-48b0-b8fd-43172307424d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048, device = \"cpu\") -> Transformer:\n",
    "\n",
    "    # Create the embedding layers\n",
    "    input_embed = InputEmbeddings(d_model, src_vocab_size, pretrained_weights).to(device)\n",
    "    # Create the positional encoding layers\n",
    "    pos = PositionalEncoding(d_model, src_seq_len, dropout).to(device)\n",
    "\n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout).to(device)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout).to(device)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout).to(device)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout).to(device)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout).to(device)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout).to(device)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout).to(device)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks)).to(device)\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks)).to(device)\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size).to(device)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, input_embed, pos, projection_layer, device=device).to(device)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab0ca881-55df-42a9-978e-9817aca92cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Transformer' object has no attribute 'tgt_pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m responseSet_tokenized\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mencode(encoder_input, encoder_mask) \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mdecode(encoder_output, encoder_mask, decoder_input, decoder_mask) \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m proj_output \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mproject(decoder_output)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (B, seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(proj_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, VOCAB_SIZE), label\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[22], line 43\u001b[0m, in \u001b[0;36mTransformer.decode\u001b[0;34m(self, encoder_output, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoder_output: torch\u001b[38;5;241m.\u001b[39mTensor, src_mask: torch\u001b[38;5;241m.\u001b[39mTensor, tgt: torch\u001b[38;5;241m.\u001b[39mTensor, tgt_mask: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# (batch, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_embed(tgt)\n\u001b[0;32m---> 43\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_pos(tgt)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, encoder_output, src_mask, tgt_mask)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Transformer' object has no attribute 'tgt_pos'"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "transformer = build_transformer( src_vocab_size = VOCAB_SIZE, \n",
    "                                tgt_vocab_size = VOCAB_SIZE, src_seq_len = MAX_LENGTH, \n",
    "                                tgt_seq_len = MAX_LENGTH, d_model = 252, N = 8 , h=6, dropout=0.1, device=device).to(device)\n",
    "label = torch.tensor(label).to(device)\n",
    "# Loss and optimizer\n",
    "ignore_tokens = [PADDING_TOKEN, UNK_TOKEN]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PADDING_TOKEN).to(device)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001,  eps=1e-9)\n",
    "num_epochs = 100\n",
    "for epoch in range(500):\n",
    "    print(epoch)\n",
    "    losses = 0\n",
    "    encoder_mask = transformer.make_src_mask(contextSet_tokenized).to(device)\n",
    "    decoder_mask = transformer.make_trg_mask(responseSet_tokenized).to(device)\n",
    "    encoder_input = contextSet_tokenized.to(device)\n",
    "    decoder_input = responseSet_tokenized.to(device)\n",
    "    encoder_output = transformer.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "    decoder_output = transformer.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "    proj_output = transformer.project(decoder_output).to(device) # (B, seq_len, vocab_size)\n",
    "    loss = criterion(proj_output.view(-1, VOCAB_SIZE), label.view(-1)).to(device)\n",
    "    loss.backward()\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    losses += loss\n",
    "    # Inside your loop\n",
    "    \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        logits = proj_output  # Assuming proj_output is your logits tensor\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Apply softmax along the last dimension\n",
    "        predicted_classes = torch.argmax(probabilities, dim=-1)  # Find the index of the class with the highest probability\n",
    "\n",
    "        # Convert tensors to numpy arrays for inspection\n",
    "        #logits_array = logits.numpy()        \n",
    "        probabilities_array = probabilities.cpu().numpy()\n",
    "        predicted_classes_array = predicted_classes.cpu().numpy()\n",
    "        sent = \"\"\n",
    "        print(predicted_classes_array)\n",
    "        for i in predicted_classes_array[0]:\n",
    "            if i<5:\n",
    "                if i == 1:\n",
    "                    sent = sent + \"<UNK>\"\n",
    "                    sent = sent + \" \"\n",
    "                if i == 3:\n",
    "                    sent = sent + \"<EOS>\"\n",
    "                    sent = sent + \" \"\n",
    "                continue\n",
    "            for word in tokens:\n",
    "                if tokens[word] == i:\n",
    "                    sent = sent + word\n",
    "                    sent = sent + \" \"\n",
    "        print(\"\")\n",
    "        print(\"Loss: \")\n",
    "        print(losses.item())\n",
    "        print(\"\")\n",
    "        print(\"True Response: \")\n",
    "        print(\"\")\n",
    "        print(responseSet[0])\n",
    "        print(\"\")\n",
    "        print(\"Predicted Response: \")\n",
    "        print(\"\")\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e127dc15-18cf-4b5c-9cbb-fcf4d123acfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've felt this way for two years. I feel so much better now then I did when it started, but it is still there in the back of my mind at all times.\n"
     ]
    }
   ],
   "source": [
    "print(contextSet[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
