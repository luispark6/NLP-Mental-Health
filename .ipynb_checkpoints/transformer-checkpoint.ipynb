{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca86dc3-6a8c-46f6-a249-4a7fdcb4acee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 520, 252])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from torch import nn, Tensor\n",
    "\n",
    "MAX_LENGTH = 520\n",
    "tokens = json.load(open(\"tokens.txt\"))\n",
    "model = torch.load(\"embedding_model\")\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"mental_health.csv\")\n",
    "orig_dataset = pd.read_csv(file_path)\n",
    "orig_dataset = orig_dataset.to_numpy()\n",
    "trainingSet =  orig_dataset[np.random.choice(orig_dataset.shape[0], 4, replace=True)] #extract training set\n",
    "contextSet = [trainingSet[i][0] for i in range(len(trainingSet))] \n",
    "responseSet = [trainingSet[i][1] for i in range(len(trainingSet))]\n",
    "#tokenizing the context and response set, also 0 is special token for unknown word\n",
    "contextSet_tokenized = [[tokens[word] if word in tokens else 0 for word in example.split()] \n",
    "                         for example in contextSet ]\n",
    "responseSet_tokenized = [[tokens[word] if word in tokens else 0 for word in example.split()] \n",
    "                         for example in responseSet ]\n",
    "\n",
    "#convert token to input embedding for context and response set filled with padding\n",
    "#tokens if end of sentence. 1 is special token for padding \n",
    "contextSet_embedding = []        \n",
    "for context in contextSet_tokenized:\n",
    "    contextEmbedding = []\n",
    "    for i in range(MAX_LENGTH):\n",
    "        if i>= len(context):\n",
    "            contextEmbedding.append(model[\"embeddings.weight\"][1])\n",
    "            continue\n",
    "        contextEmbedding.append(model[\"embeddings.weight\"][context[i]])\n",
    "    contextEmbedding = torch.stack(contextEmbedding)\n",
    "    contextSet_embedding.append(contextEmbedding[:])\n",
    "\n",
    "contextSet_embedding = torch.stack(contextSet_embedding)\n",
    "\n",
    "responseSet_embedding = []        \n",
    "for response in responseSet_tokenized:\n",
    "    responseEmbedding = []\n",
    "    for i in range(MAX_LENGTH):\n",
    "        if i>= len(response):\n",
    "            responseEmbedding.append(model[\"embeddings.weight\"][1])\n",
    "            continue\n",
    "        responseEmbedding.append(model[\"embeddings.weight\"][response[i]])\n",
    "    responseEmbedding = torch.stack(responseEmbedding)\n",
    "    responseSet_embedding.append(responseEmbedding[:])\n",
    "    \n",
    "responseSet_embedding = torch.stack(responseSet_embedding)\n",
    "print(responseSet_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba420137-3b54-46c9-83a5-84b855b7a7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37b740b8-1a7c-4354-b055-ca984b9e2779",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.7979,  0.9362, -0.0986,  ..., -0.3799,  0.0000, -0.0308],\n",
       "         [ 0.6926,  1.7718,  0.4771,  ..., -0.5516, -1.2003,  1.9489],\n",
       "         [-0.2971,  1.0521, -0.0362,  ...,  3.5024,  0.3547, -0.2309],\n",
       "         ...,\n",
       "         [ 0.0715,  1.0004,  1.2058,  ...,  2.0074,  0.7355,  1.0923],\n",
       "         [ 0.0715,  1.0004,  1.2058,  ...,  0.0000,  0.7355,  1.0923],\n",
       "         [ 0.0715,  0.0000,  1.2058,  ...,  2.0074,  0.7355,  1.0923]],\n",
       "\n",
       "        [[-0.3776,  0.8181, -1.9062,  ...,  0.0000,  1.0079,  0.0000],\n",
       "         [ 0.7852,  1.5502,  1.5748,  ...,  2.7400, -1.3242,  0.5876],\n",
       "         [ 0.5004,  2.2827,  0.0000,  ...,  0.6154,  2.5610,  1.1702],\n",
       "         ...,\n",
       "         [ 1.0065,  0.4896,  2.0961,  ...,  0.0000,  0.7356,  1.0923],\n",
       "         [ 1.0065,  0.4896,  2.0961,  ...,  2.0074,  0.7356,  1.0923],\n",
       "         [ 1.0065,  0.4896,  0.0000,  ...,  2.0074,  0.7356,  1.0923]],\n",
       "\n",
       "        [[-0.3022, -0.2446, -1.7313,  ...,  0.5140,  1.0080,  0.3587],\n",
       "         [ 1.8306, -1.6114,  2.1992,  ...,  1.2894,  0.0000,  0.0000],\n",
       "         [ 0.5757,  1.2200,  0.8421,  ...,  0.6154,  2.5612,  1.1702],\n",
       "         ...,\n",
       "         [ 1.0819, -0.5731,  2.2710,  ...,  2.0074,  0.7357,  1.0923],\n",
       "         [ 1.0819, -0.5731,  2.2710,  ...,  2.0074,  0.7357,  0.0000],\n",
       "         [ 1.0819, -0.5731,  2.2710,  ...,  2.0074,  0.7357,  1.0923]],\n",
       "\n",
       "        [[-0.0000, -2.5819,  1.1190,  ...,  1.1406,  0.2759, -3.0086],\n",
       "         [-0.7828,  0.9518, -0.8429,  ...,  0.8065,  0.2080,  0.8942],\n",
       "         [-0.0000,  0.5824,  0.1610,  ...,  0.6154,  2.5613,  1.1702],\n",
       "         ...,\n",
       "         [ 0.2283, -1.2107,  1.5899,  ...,  2.0074,  0.0000,  1.0923],\n",
       "         [ 0.2283, -1.2107,  1.5899,  ...,  2.0074,  0.7359,  1.0923],\n",
       "         [ 0.2283, -1.2107,  1.5899,  ...,  2.0074,  0.7359,  1.0923]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(252, max_len = MAX_LENGTH)\n",
    "pe.forward(contextSet_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d36e8-0be8-45bf-849a-18fca3070ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.module()):\n",
    "    def __init__(init_(self, d_model: int, h: int, dropout: float)->None:\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # linear layer for queue\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # liner layer for key\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # linear layer for \n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "                 \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        #d_k is number of embeddings per word for each head\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        # [[[[4,3],[2,1]],[[2, 5], [3,2]]]] * [[[[4,3][2,5]],[[2,1], [3,2]]]] = \n",
    "        #[[[[0.7, 0.3], [0.1, 0.9]], [0.8, 0.2], [0.4, 0.6]]]\n",
    "        #basically saying the first word of the first half of the embedding(for head 1) is related to\n",
    "        #to its own word by 0.7 and related to the first half embedding of the second word by 0.3\n",
    "        #also saying the second word of the first half of the embedding(for head 1) is related to\n",
    "        #to its own word by 0.9 and related to the first half embedding of the first word by 0.1\n",
    "        #essentially a attention matrix for each part of the embedding for the head\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        #[[[4,3,2,1] , [2,5,3,2]]]\n",
    "        #3rd dimension represents word embeddings of a word\n",
    "        #2nd dimension represent each word embedding in an example\n",
    "        #1st dimension represents each example containing each word embedding\n",
    "        # * Transform the dimensions using .view *\n",
    "        # [[[[4,3],[2,1]],[[2, 5], [3,2]]]]\n",
    "        #4th dimenension represents the size of the split embeddings for each multi-head attention\n",
    "        #3rd dimension represents the number of heads. Note that head times the size of split embeddings equals original embedding dimension\n",
    "        #2nd dimension represents sequence length\n",
    "        #1st dimension represents example lenght\n",
    "        # * Transponse dimensions 1,2 *\n",
    "        # [[[[4,3][2,5]],[[2,1], [3,2]]]]\n",
    "        # 4th dimension represents the size of the split embeddings for each multi-head attention\n",
    "        # 3rd dimension represents the length of the sequence. Contains each word with its associated head\n",
    "        # 2nd dimension represents the number of heads\n",
    "        # 1st dimension is the number of examples\n",
    "        #[[[[word 1 with first half embeddings][word 2 with first half embedding]],[[word 1 with second half], [word 2 with second half]]]]        \n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        #pass combined heads to last linear layer\n",
    "        return self.w_o(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc1610c-d6f6-4ef8-975d-223a4c8ac77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b4e705-6736-4c6b-ba6b-6bcf42c4f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
