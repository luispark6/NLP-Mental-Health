{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca86dc3-6a8c-46f6-a249-4a7fdcb4acee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 520, 252])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import copy\n",
    "from torch import nn, Tensor\n",
    "\n",
    "PADDING_TOKEN = 0\n",
    "UNK_TOKEN = 1\n",
    "SOS_TOKEN = 2\n",
    "EOS_TOKEN = 3\n",
    "MAX_LENGTH = 520\n",
    "tokens = json.load(open(\"tokens.txt\"))\n",
    "model = torch.load(\"embedding_model\")\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"mental_health.csv\")\n",
    "orig_dataset = pd.read_csv(file_path)\n",
    "orig_dataset = orig_dataset.to_numpy()\n",
    "trainingSet =  orig_dataset[np.random.choice(orig_dataset.shape[0], 4, replace=True)] #extract training set\n",
    "contextSet = [trainingSet[i][0] for i in range(len(trainingSet))] \n",
    "responseSet = [trainingSet[i][1] for i in range(len(trainingSet))]\n",
    "#tokenizing the context and response set, also 1 is special token for unknown word\n",
    "contextSet_tokenized = [[tokens[word] if word in tokens else UNK_TOKEN for word in example.split()] \n",
    "                         for example in contextSet ]\n",
    "responseSet_tokenized = [[tokens[word] if word in tokens else UNK_TOKEN for word in example.split()] \n",
    "                         for example in responseSet]\n",
    "\n",
    "label =copy.deepcopy(responseSet_tokenized)\n",
    "#set up special tokens\n",
    "for i in range(len(contextSet_tokenized)):\n",
    "    while len(contextSet_tokenized[i])!=520:\n",
    "        contextSet_tokenized[i].append(PADDING_TOKEN)\n",
    "for i in range(len(responseSet_tokenized)):\n",
    "    responseSet_tokenized[i].insert(0, SOS_TOKEN)\n",
    "    label[i].append(EOS_TOKEN)\n",
    "for i in range(len(responseSet_tokenized)):\n",
    "    while len(responseSet_tokenized[i]) != 520:\n",
    "        responseSet_tokenized[i].append(PADDING_TOKEN)\n",
    "        label[i].append(PADDING_TOKEN)\n",
    "\n",
    "    \n",
    "#convert token to input embedding for context and response set filled with padding\n",
    "#tokens if end of sentence. 0 is special token for padding \n",
    "contextSet_embedding = []        \n",
    "for context in contextSet_tokenized:\n",
    "    contextEmbedding = []\n",
    "    for i in range(MAX_LENGTH):\n",
    "        if i>= len(context):\n",
    "            contextEmbedding.append(model[\"embeddings.weight\"][1])\n",
    "            continue\n",
    "        contextEmbedding.append(model[\"embeddings.weight\"][context[i]])\n",
    "    contextEmbedding = torch.stack(contextEmbedding)\n",
    "    contextSet_embedding.append(contextEmbedding[:])\n",
    "\n",
    "contextSet_embedding = torch.stack(contextSet_embedding)\n",
    "responseSet_embedding = []        \n",
    "for response in responseSet_tokenized:\n",
    "    responseEmbedding = []\n",
    "    for i in range(MAX_LENGTH):\n",
    "        responseEmbedding.append(model[\"embeddings.weight\"][response[i]])\n",
    "    responseEmbedding = torch.stack(responseEmbedding)\n",
    "    responseSet_embedding.append(responseEmbedding[:])\n",
    "\n",
    "contextSet_tokenized = torch.tensor(contextSet_tokenized)\n",
    "responseSet_tokenized = torch.tensor(responseSet_tokenized)\n",
    "\n",
    "responseSet_embedding = torch.stack(responseSet_embedding)\n",
    "print(responseSet_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba420137-3b54-46c9-83a5-84b855b7a7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "764d36e8-0be8-45bf-849a-18fca3070ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float)->None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # linear layer for queue\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # liner layer for key\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # linear layer for \n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "                 \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        #d_k is number of embeddings per word for each head\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        # [[[[4,3],[2,1]],[[2, 5], [3,2]]]] * [[[[4,3][2,5]],[[2,1], [3,2]]]] = \n",
    "        #[[[[0.7, 0.3], [0.1, 0.9]], [0.8, 0.2], [0.4, 0.6]]]\n",
    "        #basically saying the first word of the first half of the embedding(for head 1) is related to\n",
    "        #to its own word by 0.7 and related to the first half embedding of the second word by 0.3\n",
    "        #also saying the second word of the first half of the embedding(for head 1) is related to\n",
    "        #to its own word by 0.9 and related to the first half embedding of the first word by 0.1\n",
    "        #essentially a attention matrix for each part of the embedding for the head\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        #if mask is true, then make all appropriate masked attentions scores to very low value\n",
    "        #so that the soft max will ignore their scores\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        #[[[4,3,2,1] , [2,5,3,2]]]\n",
    "        #3rd dimension represents word embeddings of a word\n",
    "        #2nd dimension represent each word embedding in an example\n",
    "        #1st dimension represents each example containing each word embedding\n",
    "        # * Transform the dimensions using .view *\n",
    "        # [[[[4,3],[2,1]],[[2, 5], [3,2]]]]\n",
    "        #4th dimenension represents the size of the split embeddings for each multi-head attention\n",
    "        #3rd dimension represents the number of heads. Note that head times the size of split embeddings equals original embedding dimension\n",
    "        #2nd dimension represents sequence length\n",
    "        #1st dimension represents example lenght\n",
    "        # * Transponse dimensions 1,2 *\n",
    "        # [[[[4,3][2,5]],[[2,1], [3,2]]]]\n",
    "        # 4th dimension represents the size of the split embeddings for each multi-head attention\n",
    "        # 3rd dimension represents the length of the sequence. Contains each word with its associated head\n",
    "        # 2nd dimension represents the number of heads\n",
    "        # 1st dimension is the number of examples\n",
    "        #[[[[word 1 with first half embeddings][word 2 with first half embedding]],[[word 1 with second half], [word 2 with second half]]]]        \n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        #pass combined heads to last linear layer\n",
    "        return self.w_o(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afc1610c-d6f6-4ef8-975d-223a4c8ac77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        #the output of the self.linear1 layer is the input of a relu layer\n",
    "        #the output of that is the input of linear layer 2\n",
    "     \n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb762885-8309-40e1-978b-62b8cdacbe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the data for stabilized training\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53b4e705-6736-4c6b-ba6b-6bcf42c4f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "    def forward(self, x, sublayer):\n",
    "        # in the paper, you would typically add x to the sublayer and then normalize the output\n",
    "        #of the sublayer, but in this case, we normalize before passing it in to the sublayer\n",
    "        #the .norm part is the normalizing part in ADD and NORM, and the addition of x is the \n",
    "        #ADD part of ADD and NORM\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89ad844d-9fca-4fbf-9343-e1900f44d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        #encoder block contains 2 residual connections\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        #Passes self attention block to residuals. Residuals will perform the attention block\n",
    "        #and also perform the ADD and NORM\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        #passes the output of the previous residual_connection layer('x'). Then, passes in a feed_forward_block\n",
    "        #Residual connection will perform the feed forward and ADD and NORM\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a51e5746-97a1-468f-83c9-60206377b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        #layers of encoder blocks\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        #perform a forward method on every encoder block. The output of each encoder\n",
    "        #becomes the input of the new encoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        #normalize the final output\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e87e855-5461-4bf9-be13-049368037b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "    \n",
    "    #src mask are for the encoder output. Do not want to attend to padding tokens\n",
    "    #tgt mask are for decoder input. Doesn't let you look into the future\n",
    "    #typically these just always be true I think\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8345ccd-9a72-41f3-a9f7-7c491fb67580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0b49481-6256-440b-a008-d267f62b4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear layer where output represents all words. We are going to soft max this \n",
    "#in the future. \n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c306bd0f-0a09-4729-ab51-c5d139ce52bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed, tgt_embed, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer, src_pad_indx = 0) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "        self.src_pad_idx = src_pad_indx\n",
    "        self._pad_idx = src_pad_indx\n",
    "        self.device = \"cpu\"\n",
    "        \n",
    "    \n",
    "    def make_src_mask(self, src_tokens):\n",
    "        src_mask = (src_tokens != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "    \n",
    "    def make_trg_mask(self, trg_tokens):\n",
    "        N, trg_len = trg_tokens.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "897029fa-3e85-48b0-b8fd-43172307424d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_transformer(src_embed, tgt_embed, src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, dropout, src_seq_len)\n",
    "    tgt_pos = PositionalEncoding(d_model, dropout,tgt_seq_len)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab0ca881-55df-42a9-978e-9817aca92cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes:\n",
      "[0 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1\n",
      " 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 1\n",
      " 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0\n",
      " 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "Ideally you and your boyfriend will reach a balance point where you and your boyfriend are each happy with the level of involvement you have with your former boyfriend.Start w a discussion with your current boyfriend as to what specifically he doesn't like about your friendship with your former boyfriend.It is possible that you can answer his concerns as well he can find out from you more as to what the friendship is all about.As a therapist, I've never seen a former romantic relationship become only a friendship.   As sincere as you may be in your intention to only keep the friendship with the former relationship partner, if  your former boyfriend secretly has romantic feelings for you, then at best, you've got an unclear friendship with this person.The obvious possibility is to socialize together with your current boyfriend and your former one. If neither guy would go for this, then this would show there is an undercurrent of competition for your romantic attention.Basically make your romantic partner's feelings and your own, the major considerations and discuss from this perspective.\n",
      "\n",
      "Predicted Classes:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "Ideally you and your boyfriend will reach a balance point where you and your boyfriend are each happy with the level of involvement you have with your former boyfriend.Start w a discussion with your current boyfriend as to what specifically he doesn't like about your friendship with your former boyfriend.It is possible that you can answer his concerns as well he can find out from you more as to what the friendship is all about.As a therapist, I've never seen a former romantic relationship become only a friendship.   As sincere as you may be in your intention to only keep the friendship with the former relationship partner, if  your former boyfriend secretly has romantic feelings for you, then at best, you've got an unclear friendship with this person.The obvious possibility is to socialize together with your current boyfriend and your former one. If neither guy would go for this, then this would show there is an undercurrent of competition for your romantic attention.Basically make your romantic partner's feelings and your own, the major considerations and discuss from this perspective.\n",
      "\n",
      "Predicted Classes:\n",
      "[ 1 10 14  1  1 10  1  1  1 31  1  1  1  1  1 10  1  1  1  1  1 10  1  6\n",
      "  1  6  1  1  1  6  1  1 31  1  1  6 10  1  1  1  6  6  1  1  1  1  6  1\n",
      "  1  6  1  6  1  1  1  1  1  1  1  1  1  6  1  1  1  1  1  1 13  1  1  1\n",
      " 31  6  1 10  1  6  1  1  6  1  6  1  1  1  1  1  6  1  1  0  1  1  1  1\n",
      "  1 10  1  1  6  1  1  1  1  6  1 10  1  1  6 10  1  1  6  1  1  1  1  1\n",
      "  1  1  6  1 31  6  1  1  6 10 10  6  1  1  1 27  1  1  6  1  1  1  1  1\n",
      "  1  6  1  1  1  1  1  1  1 10  1  6  1 10  1  1  1  1  1  6  1  1  6  6\n",
      "  1  1  1  1  1  1  6  1  1  1  1  6  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Ideally you and your boyfriend will reach a balance point where you and your boyfriend are each happy with the level of involvement you have with your former boyfriend.Start w a discussion with your current boyfriend as to what specifically he doesn't like about your friendship with your former boyfriend.It is possible that you can answer his concerns as well he can find out from you more as to what the friendship is all about.As a therapist, I've never seen a former romantic relationship become only a friendship.   As sincere as you may be in your intention to only keep the friendship with the former relationship partner, if  your former boyfriend secretly has romantic feelings for you, then at best, you've got an unclear friendship with this person.The obvious possibility is to socialize together with your current boyfriend and your former one. If neither guy would go for this, then this would show there is an undercurrent of competition for your romantic attention.Basically make your romantic partner's feelings and your own, the major considerations and discuss from this perspective.\n",
      "is are is a is is to to to a to is to to to to to to you a to is to to to to is to to is to is to to a to to is is to your to to is to is to to to to to \n",
      "Predicted Classes:\n",
      "[ 1  1 14  1  1  6  1  1  1  6  1 13 14  1  1  6  1  1  1 27  1  6  1  6\n",
      " 14  1 27  1  1  6  1  1  1 27  1  1  6 13  1  1  1  1  1  1 27  1  1 27\n",
      "  1  6  6  6  1  1  7  1  1  1  1 13  1  1  1  1  1  1  7  1 13  1  1  1\n",
      "  1  6  1  6  1  1  1  1  6  1  1  1  1  1  1  1  6  1  1 13 14  1  1  1\n",
      "  1  1  1  1  6  1  6 27  1  1  1  1  1  1  1  1  6  1  1  1  1  1  1  1\n",
      "  6  1  6  1  6  6 27  1  6  1  1  6  1  1  1 27  1  1  6  1  1  6  6  1\n",
      "  6  6  1  1  1  1  1  1  1  6  1  6  1  6  1  1  1  1  6  6  1  1  1 10\n",
      "  1  1  1  6  1  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Ideally you and your boyfriend will reach a balance point where you and your boyfriend are each happy with the level of involvement you have with your former boyfriend.Start w a discussion with your current boyfriend as to what specifically he doesn't like about your friendship with your former boyfriend.It is possible that you can answer his concerns as well he can find out from you more as to what the friendship is all about.As a therapist, I've never seen a former romantic relationship become only a friendship.   As sincere as you may be in your intention to only keep the friendship with the former relationship partner, if  your former boyfriend secretly has romantic feelings for you, then at best, you've got an unclear friendship with this person.The obvious possibility is to socialize together with your current boyfriend and your former one. If neither guy would go for this, then this would show there is an undercurrent of competition for your romantic attention.Basically make your romantic partner's feelings and your own, the major considerations and discuss from this perspective.\n",
      "are to to you are to your to to are your to your to you your your to to to and you and you to to to to you are to to your to to to to to your to to your to to to to to to to to to to is to \n",
      "Predicted Classes:\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 27  1  1  1  1\n",
      " 14  1 27  1  1  1  1  1  1 27  1  1  1  1  1  1  1  1  1  1  1  1  1 27\n",
      "  1  1  1  1  1  1  7  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  7  1  1  1\n",
      "  1  1  1  1  1  1  1 27  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 27  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1 27  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Ideally you and your boyfriend will reach a balance point where you and your boyfriend are each happy with the level of involvement you have with your former boyfriend.Start w a discussion with your current boyfriend as to what specifically he doesn't like about your friendship with your former boyfriend.It is possible that you can answer his concerns as well he can find out from you more as to what the friendship is all about.As a therapist, I've never seen a former romantic relationship become only a friendship.   As sincere as you may be in your intention to only keep the friendship with the former relationship partner, if  your former boyfriend secretly has romantic feelings for you, then at best, you've got an unclear friendship with this person.The obvious possibility is to socialize together with your current boyfriend and your former one. If neither guy would go for this, then this would show there is an undercurrent of competition for your romantic attention.Basically make your romantic partner's feelings and your own, the major considerations and discuss from this perspective.\n",
      "your are your your your and and your your your \n",
      "Predicted Classes:\n",
      "[  1   1  24  27   1   1   1   1   1   1   1  13  14  27   1   1   1   1\n",
      "   1  27   1   1   1   1  24   6  27   1   1   1   1   1   1  27   1   1\n",
      "   1  13   1   1   1   1   1   1  27   1   1  27   1   1   1   8   1   1\n",
      "  14   1   1   1   1  13   1   1   1   1   1   1  76   1  13   1   1   1\n",
      "   1   6   1   1   1   1  27   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1  13  76   1 156   1   1   1   1   1   1   1   1  27   1   1   1   1\n",
      "  13   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "  27   1   1   1   1   8   1   1   1  27   1   1   1  27   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   6   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1  27   1   1   1   1   1  27   1   1   1   1\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "Ideally you and your boyfriend will reach a balance point where you and your boyfriend are each happy with the level of involvement you have with your former boyfriend.Start w a discussion with your current boyfriend as to what specifically he doesn't like about your friendship with your former boyfriend.It is possible that you can answer his concerns as well he can find out from you more as to what the friendship is all about.As a therapist, I've never seen a former romantic relationship become only a friendship.   As sincere as you may be in your intention to only keep the friendship with the former relationship partner, if  your former boyfriend secretly has romantic feelings for you, then at best, you've got an unclear friendship with this person.The obvious possibility is to socialize together with your current boyfriend and your former one. If neither guy would go for this, then this would show there is an undercurrent of competition for your romantic attention.Basically make your romantic partner's feelings and your own, the major considerations and discuss from this perspective.\n",
      "can your you are your your can to your your you your your the are you may you to your you may very your you your the your your to your your \n",
      "Predicted Classes:\n",
      "[  1   1 173  27   1   1   1   1   1   1   1  13  76 296   1   1   1 430\n",
      "   1  27   1   1   1   1  76   6  27   1   1   1   1   1   1  27   1   1\n",
      "   1  13   1   1   1   1   1  42  27   1   1  27   1   1   1   6   1   1\n",
      "  24   1   1   1   1   0   1   1   1   1   1   0  14   1  13   1   1   1\n",
      "   1   6   1   1   1   1   1   1   1   1   1   1   1 393   1   1   1   1\n",
      "   1  13  76  36 156   1   1   1   1   1   1   1   1  27   1   1   1   1\n",
      "  13   1   1   1   1   1   1   7 484 122   1  80   1   1   1   1   1   1\n",
      "  27   1   1   1   1   6   1   1   0  27   1   1   1  27   1   1   1  13\n",
      "   1   1  20   1  39   1   1   1  20   1   1   8   1   1   1   1 484   1\n",
      "   1   1   1   1   1   1   7  27   1   1   1   1   1  27   1   1   1   1\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "Ideally you and your boyfriend will reach a balance point where you and your boyfriend are each happy with the level of involvement you have with your former boyfriend.Start w a discussion with your current boyfriend as to what specifically he doesn't like about your friendship with your former boyfriend.It is possible that you can answer his concerns as well he can find out from you more as to what the friendship is all about.As a therapist, I've never seen a former romantic relationship become only a friendship.   As sincere as you may be in your intention to only keep the friendship with the former relationship partner, if  your former boyfriend secretly has romantic feelings for you, then at best, you've got an unclear friendship with this person.The obvious possibility is to socialize together with your current boyfriend and your former one. If neither guy would go for this, then this would show there is an undercurrent of competition for your romantic attention.Basically make your romantic partner's feelings and your own, the major considerations and discuss from this perspective.\n",
      "start your you may understand happy your may to your your you about your your to can are you to only you may have very your you and you, then what your to your your you be this be the you, and your your \n",
      "Predicted Classes:\n",
      "[  1   1   7  27   1   1 130   1   1   1   1  13   7  27   1   1   1 430\n",
      "   1  27   1   1   1   1  76   6  27   1   1   1   1   1   1  27   1   1\n",
      "   1  13   1   1   1   1   1  42  27   1   1  27   1   1   1   6   1   1\n",
      "  24   1   1   1   1  13   1   1   1   1   1  13 173   1   6   1   1   1\n",
      "   1   6   1   1   1   1  23   1   1   1   1   1   1 393   1   1   1   1\n",
      "   1  13  24  36 156   1   1   1   1   1   1   1   1  27   1   1   1   1\n",
      "  13   1   1   1   1   1   1   7 484 122   1  80   1   1   1   1   1   1\n",
      "  27   1   1   1   1   6   1   1   0  27   1   1   1  27   1   1   1   1\n",
      "   1   1  20   1 484   1   1   1   1   1   1   6   1   1   1   1  39   1\n",
      "   1   1   1   1   1   1   1  27   1   1   1   1   1  27   1   1   1   1\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "Ideally you and your boyfriend will reach a balance point where you and your boyfriend are each happy with the level of involvement you have with your former boyfriend.Start w a discussion with your current boyfriend as to what specifically he doesn't like about your friendship with your former boyfriend.It is possible that you can answer his concerns as well he can find out from you more as to what the friendship is all about.As a therapist, I've never seen a former romantic relationship become only a friendship.   As sincere as you may be in your intention to only keep the friendship with the former relationship partner, if  your former boyfriend secretly has romantic feelings for you, then at best, you've got an unclear friendship with this person.The obvious possibility is to socialize together with your current boyfriend and your former one. If neither guy would go for this, then this would show there is an undercurrent of competition for your romantic attention.Basically make your romantic partner's feelings and your own, the major considerations and discuss from this perspective.\n",
      "and your reach you and your happy your may to your your you about your your to can you you start to to never only you can have very your you and you, then what your to your your be you, to this your your \n",
      "Predicted Classes:\n",
      "[  1   1   7  27   1   1 130  31   1   1   1  13   7  27   1   1   1 430\n",
      "   1  27   1   1   1   1   7   6  27   1   1   1   1   1   1  27   1   1\n",
      "   1   6   1   1   1   1  86  42  27   1   1  27   1   1   1   8   1   1\n",
      "  76  52   1   1   1  13   1   1 318   1   1   1   7   1   6   1   1   1\n",
      "   1   8   1   1   1   1  23   1   1   1   1   1   1 393   1   1   1   1\n",
      "   1  13   7  36 156   1   1   1   1   1   1   1   1  27   1   1   1   1\n",
      "  13   1   1   1   1   1   1   7 484   1 109  80   1   1   1   1   1   1\n",
      "  27   1   1   1   1   8   1   1   6  27   1   1   1  27   1   1   1  13\n",
      "   1   1   1   1 484 122 109   1   1   1 174   6   1   1   1   1 484   1\n",
      "   1   1   1   1   1   1   7  27   1   1   1   1   1  27   1   1   1   1\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "Ideally you and your boyfriend will reach a balance point where you and your boyfriend are each happy with the level of involvement you have with your former boyfriend.Start w a discussion with your current boyfriend as to what specifically he doesn't like about your friendship with your former boyfriend.It is possible that you can answer his concerns as well he can find out from you more as to what the friendship is all about.As a therapist, I've never seen a former romantic relationship become only a friendship.   As sincere as you may be in your intention to only keep the friendship with the former relationship partner, if  your former boyfriend secretly has romantic feelings for you, then at best, you've got an unclear friendship with this person.The obvious possibility is to socialize together with your current boyfriend and your former one. If neither guy would go for this, then this would show there is an undercurrent of competition for your romantic attention.Basically make your romantic partner's feelings and your own, the major considerations and discuss from this perspective.\n",
      "and your reach a you and your happy your and to your your to like about your your the may find you answer and to the never only you and have very your you and you, at what your the to your your you you, then at specific to you, and your your \n",
      "Predicted Classes:\n",
      "[  1   1   7  27   1   1 130  31   1   1 201  13   7  27   1   1   1 430\n",
      "  28  27   1   1   1   1  14  31  27   1   1   1   1   1   1  27   1   1\n",
      "   1   6   1   1   1   1  86  42  27   1   1  27   1   1   1   8   9   1\n",
      "   7  52   1   1   1   6   1   1 318   1  11  13   7   1  13   1   1   1\n",
      "   1   8   1   1   1   1  23   1   1   1   1   1   1 393   1   1   1   1\n",
      "   1   6  76  36 156   1   1   1   1   1   1   1   1  27   1   1 403   1\n",
      "  13   1   1   1   1   1   1  41 484 122 109  80   1   1   1   1   1   1\n",
      "  27   1   1   1   1   8   1   1   6  27   1   1   1  27   1   1   1  13\n",
      "   1   1  20   1 484 122  39   1  20   1 174   8   1   1   1   1 484   1\n",
      "   1   1  27   1   1   1   7  27   1   1   1   1   1  27 108  13   1   1\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "Ideally you and your boyfriend will reach a balance point where you and your boyfriend are each happy with the level of involvement you have with your former boyfriend.Start w a discussion with your current boyfriend as to what specifically he doesn't like about your friendship with your former boyfriend.It is possible that you can answer his concerns as well he can find out from you more as to what the friendship is all about.As a therapist, I've never seen a former romantic relationship become only a friendship.   As sincere as you may be in your intention to only keep the friendship with the former relationship partner, if  your former boyfriend secretly has romantic feelings for you, then at best, you've got an unclear friendship with this person.The obvious possibility is to socialize together with your current boyfriend and your former one. If neither guy would go for this, then this would show there is an undercurrent of competition for your romantic attention.Basically make your romantic partner's feelings and your own, the major considerations and discuss from this perspective.\n",
      "and your reach a where you and your happy with your are a your your to like about your your the that and find to answer of you and you the never only to may have very your become you for you, then at what your the to your your you be you, then this be specific the you, your and your your from you \n"
     ]
    }
   ],
   "source": [
    "transformer = build_transformer(contextSet_embedding, responseSet_embedding, src_vocab_size = 589, \n",
    "                                tgt_vocab_size = 589, src_seq_len = 520, \n",
    "                                tgt_seq_len = 520, d_model = 252, N =6, h=6, dropout=0.1)\n",
    "\n",
    "label = label.clone().detach()\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(transformer.parameters(), eps=1e-9)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    losses = 0\n",
    "    for batch in range(len(contextSet_embedding)):\n",
    "        decoder_mask = transformer.make_src_mask(contextSet_tokenized)\n",
    "        encoder_mask = transformer.make_trg_mask(responseSet_tokenized)\n",
    "        encoder_input = contextSet_embedding\n",
    "        decoder_input = responseSet_embedding\n",
    "        encoder_output = transformer.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "        decoder_output = transformer.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "        proj_output = transformer.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "        loss = criterion(proj_output.view(-1, 589), label.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        losses += loss\n",
    "    # Inside your loop\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        logits = proj_output  # Assuming proj_output is your logits tensor\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Apply softmax along the last dimension\n",
    "        predicted_classes = torch.argmax(probabilities, dim=-1)  # Find the index of the class with the highest probability\n",
    "\n",
    "        # Convert tensors to numpy arrays for inspection\n",
    "        logits_array = logits.numpy()\n",
    "        probabilities_array = probabilities.numpy()\n",
    "        predicted_classes_array = predicted_classes.numpy()        \n",
    "        print(predicted_classes_array[0])\n",
    "        sent = \"\"\n",
    "        for i in predicted_classes_array[0]:\n",
    "            if i<5:\n",
    "                continue\n",
    "            for word in tokens:\n",
    "                if tokens[word] == i:\n",
    "                    sent = sent + word\n",
    "                    sent = sent + \" \"\n",
    "        print(responseSet[0])\n",
    "        print(sent)\n",
    "            \n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
