{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241b1e5f-d6bf-4dbe-bd2c-be380a8fe4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Vocab Size: 10439\n",
      "Total Training Size: 640\n",
      "Batch Size: 16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import copy\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "SRC_VOCAB_SIZE = 3\n",
    "TGT_VOCAB_SIZE = 3\n",
    "D_MODEL = 252\n",
    "PADDING_TOKEN = 0\n",
    "UNK_TOKEN = 1\n",
    "SOS_TOKEN = 2\n",
    "EOS_TOKEN = 3\n",
    "MAX_LENGTH = 200\n",
    "BATCH_SIZE = 0\n",
    "src_tokens = {}\n",
    "tgt_tokens = {}\n",
    "\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"\", \"mental_health.csv\")\n",
    "orig_dataset = pd.read_csv(file_path) #read csv file as pandas object\n",
    "orig_dataset = orig_dataset.to_numpy()\n",
    "for example in range(len(orig_dataset)): \n",
    "    for cont_response in range(2): #context than response\n",
    "        if type(orig_dataset[example][cont_response]) == float: #NaN values\n",
    "            continue\n",
    "        cur_Sentence = orig_dataset[example][cont_response].split() #seperate by word\n",
    "        if cont_response == 0:\n",
    "            for word in cur_Sentence:\n",
    "                if word not in src_tokens: src_tokens[word] = 1\n",
    "                else: src_tokens[word] +=1\n",
    "        else:\n",
    "            for word in cur_Sentence:\n",
    "                if word not in tgt_tokens: tgt_tokens[word] = 1\n",
    "                else: tgt_tokens[word] +=1\n",
    "            \n",
    "SRC_TOKENS = {}\n",
    "TGT_TOKENS = {}\n",
    "token = 4\n",
    "for word in src_tokens:\n",
    "    if src_tokens[word]>2:\n",
    "        SRC_TOKENS[word] = token\n",
    "        token+=1\n",
    "        SRC_VOCAB_SIZE+=1\n",
    "        \n",
    "token = 4\n",
    "for word in tgt_tokens:\n",
    "    if tgt_tokens[word]>2:\n",
    "        TGT_TOKENS[word] = token\n",
    "        token+=1\n",
    "        TGT_VOCAB_SIZE+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "if not os.path.exists('context.txt'):\n",
    "    unfiltered_trainingSet =  orig_dataset[np.random.choice(orig_dataset.shape[0], 1000, replace=True)] #extract training set\n",
    "    trainingSet = []\n",
    "    for example in unfiltered_trainingSet:\n",
    "        if type(example[0]) is not float and type(example[1]) is not float:\n",
    "            if len(example[0].split())<200 and len(example[1].split())<200:\n",
    "                trainingSet.append(example)\n",
    "\n",
    "    contextSet = [trainingSet[i][0] for i in range(len(trainingSet))] \n",
    "    responseSet = [trainingSet[i][1] for i in range(len(trainingSet))]\n",
    "    contextSet_tokenized =[[SRC_TOKENS[word] if word in SRC_TOKENS else UNK_TOKEN for word in example.split()] \n",
    "                             for example in contextSet]\n",
    "\n",
    "    # print(contextSet_tokenized)\n",
    "    # for i in contextSet_tokenized[0]:\n",
    "    #     for x in SRC_TOKENS:\n",
    "    #         if SRC_TOKENS[x] == i:\n",
    "    #             print(x)\n",
    "\n",
    "    responseSet_tokenized =[[TGT_TOKENS[word] if word in TGT_TOKENS else UNK_TOKEN for word in example.split()] \n",
    "                             for example in responseSet]\n",
    "\n",
    "    # print(responseSet_tokenized)\n",
    "    # for i in responseSet_tokenized[0]:\n",
    "    #     for x in TGT_TOKENS:\n",
    "    #         if TGT_TOKENS[x] == i:\n",
    "    #             print(x)\n",
    "\n",
    "    label =copy.deepcopy(responseSet_tokenized)\n",
    "    #set up special tokens\n",
    "    for i in range(len(contextSet_tokenized)):\n",
    "        while len(contextSet_tokenized[i])!= MAX_LENGTH:\n",
    "            contextSet_tokenized[i].append(PADDING_TOKEN)\n",
    "    for i in range(len(responseSet_tokenized)):\n",
    "        responseSet_tokenized[i].insert(0, SOS_TOKEN)\n",
    "        label[i].append(EOS_TOKEN)\n",
    "    for i in range(len(responseSet_tokenized)):\n",
    "        while len(responseSet_tokenized[i]) != MAX_LENGTH:\n",
    "            responseSet_tokenized[i].append(PADDING_TOKEN)\n",
    "            label[i].append(PADDING_TOKEN)\n",
    "\n",
    "\n",
    "\n",
    "    json.dump(responseSet_tokenized, open(\"response.txt\",'w'))\n",
    "    json.dump(contextSet_tokenized, open(\"context.txt\",'w'))\n",
    "    json.dump(label, open(\"labels.txt\",'w'))\n",
    "    json.dump(SRC_TOKENS, open(\"source_tokens.txt\",'w'))\n",
    "    json.dump(TGT_TOKENS, open(\"target_tokens.txt\",'w'))\n",
    "else:\n",
    "    # Load data from files\n",
    "    with open('response.txt', 'r') as f:\n",
    "        responseSet_tokenized = json.load(f)\n",
    "\n",
    "    with open('context.txt', 'r') as f:\n",
    "        contextSet_tokenized = json.load(f)\n",
    "\n",
    "    with open('labels.txt', 'r') as f:\n",
    "        label = json.load(f)\n",
    "\n",
    "    with open('source_tokens.txt', 'r') as f:\n",
    "        SRC_TOKENS = json.load(f)\n",
    "\n",
    "    with open('target_tokens.txt', 'r') as f:\n",
    "        TGT_TOKENS = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = len(contextSet_tokenized) // 40 \n",
    "mod = len(contextSet_tokenized) % BATCH_SIZE\n",
    "if mod:\n",
    "    reduce = len(contextSet_tokenized) - (BATCH_SIZE*40)\n",
    "    responseSet_tokenized = responseSet_tokenized[: len(responseSet_tokenized)-reduce]\n",
    "    contextSet_tokenized = contextSet_tokenized[: len(contextSet_tokenized)-reduce]\n",
    "    label = label[: len(label)-reduce]\n",
    "    \n",
    "print(f\"Target Vocab Size: {TGT_VOCAB_SIZE}\")\n",
    "print(f\"Total Training Size: {len(responseSet_tokenized)}\")    \n",
    "print(f\"Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46928fd2-3a06-432f-8dde-a295aba0ee8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model = D_MODEL, seq_len = MAX_LENGTH, dropout = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6164cdf3-1de8-4e6d-abd1-59d913a2f29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float)->None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # linear layer for queue\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # liner layer for key\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # linear layer for \n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "                 \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        #d_k is number of embeddings per word for each head\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        # [[[[4,3],[2,1]],[[2, 5], [3,2]]]] * [[[[4,3][2,5]],[[2,1], [3,2]]]] = \n",
    "        #[[[[0.7, 0.3], [0.1, 0.9]], [0.8, 0.2], [0.4, 0.6]]]\n",
    "        #basically saying the first word of the first half of the embedding(for head 1) is related to\n",
    "        #to its own word by 0.7 and related to the first half embedding of the second word by 0.3\n",
    "        #also saying the second word of the first half of the embedding(for head 1) is related to\n",
    "        #to its own word by 0.9 and related to the first half embedding of the first word by 0.1\n",
    "        #essentially a attention matrix for each part of the embedding for the head\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        #if mask is true, then make all appropriate masked attentions scores to very low value\n",
    "        #so that the soft max will ignore their scores\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        #[[[4,3,2,1] , [2,5,3,2]]]\n",
    "        #3rd dimension represents word embeddings of a word\n",
    "        #2nd dimension represent each word embedding in an example\n",
    "        #1st dimension represents each example containing each word embedding\n",
    "        # * Transform the dimensions using .view *\n",
    "        # [[[[4,3],[2,1]],[[2, 5], [3,2]]]]\n",
    "        #4th dimenension represents the size of the split embeddings for each multi-head attention\n",
    "        #3rd dimension represents the number of heads. Note that head times the size of split embeddings equals original embedding dimension\n",
    "        #2nd dimension represents sequence length\n",
    "        #1st dimension represents example lenght\n",
    "        # * Transponse dimensions 1,2 *\n",
    "        # [[[[4,3][2,5]],[[2,1], [3,2]]]]\n",
    "        # 4th dimension represents the size of the split embeddings for each multi-head attention\n",
    "        # 3rd dimension represents the length of the sequence. Contains each word with its associated head\n",
    "        # 2nd dimension represents the number of heads\n",
    "        # 1st dimension is the number of examples\n",
    "        #[[[[word 1 with first half embeddings][word 2 with first half embedding]],[[word 1 with second half], [word 2 with second half]]]]        \n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        #pass combined heads to last linear layer\n",
    "        return self.w_o(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b8bdd2-62ec-4c5d-bae9-be4ef98f97e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        #the output of the self.linear1 layer is the input of a relu layer\n",
    "        #the output of that is the input of linear layer 2\n",
    "     \n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa629111-af8e-4f25-a2dd-95cdf54f5980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#normalizing the data for stabilized training\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "823a2798-2948-4a3a-9965-37b4906331ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "    def forward(self, x, sublayer):\n",
    "        # in the paper, you would typically add x to the sublayer and then normalize the output\n",
    "        #of the sublayer, but in this case, we normalize before passing it in to the sublayer\n",
    "        #the .norm part is the normalizing part in ADD and NORM, and the addition of x is the \n",
    "        #ADD part of ADD and NORM\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbafda2d-791b-45d3-b7c9-e512e589f879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        #encoder block contains 2 residual connections\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        #Passes self attention block to residuals. Residuals will perform the attention block\n",
    "        #and also perform the ADD and NORM\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        #passes the output of the previous residual_connection layer('x'). Then, passes in a feed_forward_block\n",
    "        #Residual connection will perform the feed forward and ADD and NORM\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de2ffada-3b00-4abc-8f30-1bde16e76a11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        #layers of encoder blocks\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        #perform a forward method on every encoder block. The output of each encoder\n",
    "        #becomes the input of the new encoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        #normalize the final output\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6edb335-1db7-4866-beb2-e0d739278633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "    \n",
    "    #src mask are for the encoder output. Do not want to attend to padding tokens\n",
    "    #tgt mask are for decoder input. Doesn't let you look into the future\n",
    "    #typically these just always be true I think\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e87b7238-ce56-4974-a380-6311d70163fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20b80baa-001a-4a11-a626-8e7c6df5fe8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#linear layer where output represents all words. We are going to soft max this \n",
    "#in the future. \n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46cc0ae9-3601-4ac8-aa7c-7ea9ea98b272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings ,src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer, src_pad_indx = 0, device = \"cpu\") -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "        self.src_pad_idx = src_pad_indx\n",
    "        self._pad_idx = src_pad_indx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src_tokens):\n",
    "        src_mask = (src_tokens != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "    \n",
    "    def make_trg_mask(self, trg_tokens):\n",
    "        N, trg_len = trg_tokens.shape\n",
    "        trg_pad_mask = (trg_tokens != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "        trg_pad_mask= trg_pad_mask.to(self.device)\n",
    "        trg_sub_mask= trg_sub_mask.to(self.device)\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        # print(trg_mask)\n",
    "        \n",
    "        # N, trg_len = trg_tokens.shape\n",
    "        # trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "        #     N, 1, trg_len, trg_len\n",
    "        # )\n",
    "\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b778345a-6cb1-45f5-aadf-c3ca5fd00658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048, device = \"cpu\") -> Transformer:\n",
    "\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size).to(device)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size).to(device)\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout).to(device)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout).to(device)\n",
    "\n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout).to(device)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout).to(device)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout).to(device)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout).to(device)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout).to(device)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout).to(device)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout).to(device)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks)).to(device)\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks)).to(device)\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size).to(device)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer, device=device).to(device)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bec02bd-27d3-4394-b873-2535a5f0c3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# device = \"cpu\"\n",
    "transformer = build_transformer( src_vocab_size = SRC_VOCAB_SIZE, \n",
    "                                tgt_vocab_size = TGT_VOCAB_SIZE, src_seq_len = MAX_LENGTH, \n",
    "                                tgt_seq_len = MAX_LENGTH, d_model = D_MODEL, N = 8 , h=6, dropout=0.1, device=device).to(device)\n",
    "\n",
    "split_contextSet_tokenized = [contextSet_tokenized[i: i+BATCH_SIZE] for i in range(0, len(contextSet_tokenized), BATCH_SIZE)]\n",
    "split_responseSet_tokenized = [responseSet_tokenized[i: i+BATCH_SIZE] for i in range(0, len(responseSet_tokenized), BATCH_SIZE)]\n",
    "split_label = [label[i: i+BATCH_SIZE] for i in range(0, len(label), BATCH_SIZE)]\n",
    "print(len(split_contextSet_tokenized[0]))\n",
    "split_contextSet_tokenized = torch.tensor(split_contextSet_tokenized)\n",
    "split_responseSet_tokenized = torch.tensor(split_responseSet_tokenized)\n",
    "split_label = torch.tensor(split_label)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PADDING_TOKEN).to(device)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001,  eps=1e-9)\n",
    "num_epochs = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e600195c-7251-4a55-8c1c-5c482e50b8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m split_label \u001b[38;5;241m=\u001b[39m [label[i: i\u001b[38;5;241m+\u001b[39mBATCH_SIZE] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(label), BATCH_SIZE)]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(split_contextSet_tokenized[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m---> 13\u001b[0m split_contextSet_tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_contextSet_tokenized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m split_responseSet_tokenized \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(split_responseSet_tokenized)\n\u001b[1;32m     15\u001b[0m split_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(split_label)\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(len(split_contextSet_tokenized)):\n",
    "        contextSet_tokenized= split_contextSet_tokenized[batch] \n",
    "        responseSet_tokenized= split_responseSet_tokenized[batch]\n",
    "        label = split_label[batch]\n",
    "        contextSet_tokenized =contextSet_tokenized.to(device)\n",
    "        responseSet_tokenized =responseSet_tokenized.to(device)\n",
    "        label =label.to(device)\n",
    "        losses = 0\n",
    "        encoder_mask = transformer.make_src_mask(contextSet_tokenized).to(device)\n",
    "        decoder_mask = transformer.make_trg_mask(responseSet_tokenized).to(device)\n",
    "        encoder_input = contextSet_tokenized.to(device)\n",
    "        decoder_input = responseSet_tokenized.to(device)\n",
    "        encoder_output = transformer.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "        decoder_output = transformer.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "        proj_output = transformer.project(decoder_output).to(device) # (B, seq_len, vocab_size)\n",
    "        loss = criterion(proj_output.view(-1, TGT_VOCAB_SIZE), label.view(-1)).to(device)\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        losses += loss\n",
    "    if not epoch%10:\n",
    "        print(epoch)\n",
    "        # Inside your loop\n",
    "\n",
    "\n",
    "    #     with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    #         logits = proj_output  # Assuming proj_output is your logits tensor\n",
    "    #         probabilities = F.softmax(logits, dim=-1)  # Apply softmax along the last dimension\n",
    "    #         predicted_classes = torch.argmax(probabilities, dim=-1)  # Find the index of the class with the highest probability\n",
    "\n",
    "    #         # Convert tensors to numpy arrays for inspection\n",
    "    #         #logits_array = logits.numpy()        \n",
    "    #         probabilities_array = probabilities.cpu().numpy()\n",
    "    #         predicted_classes_array = predicted_classes.cpu().numpy()\n",
    "    #         sent = \"\"\n",
    "    #         for i in predicted_classes_array[0]:\n",
    "    #             if i<4:\n",
    "    #                 if i == 1:\n",
    "    #                     sent = sent + \"<UNK>\"\n",
    "    #                     sent = sent + \" \"\n",
    "    #                 if i == 3:\n",
    "    #                     sent = sent + \"<EOS>\"\n",
    "    #                     sent = sent + \" \"\n",
    "    #                     break\n",
    "    #                 continue\n",
    "    #             for word in TGT_TOKENS:\n",
    "    #                 if TGT_TOKENS[word] == i:\n",
    "    #                     sent = sent + word\n",
    "    #                     sent = sent + \" \"\n",
    "    #         print(\"\")\n",
    "    #         print(\"Loss: \")\n",
    "    #         print(losses.item())\n",
    "    #         print(\"\")\n",
    "    #         print(\"True Response: \")\n",
    "    #         print(\"\")\n",
    "    #         print(responseSet[0])\n",
    "    #         print(\"\")\n",
    "    #         print(\"Predicted Response: \")\n",
    "    #         print(\"\")\n",
    "    #         print(sent)\n",
    "print(\"Training Finished\")\n",
    "torch.save(transformer.state_dict(), 'transformer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a10af45-95eb-4eb4-821e-59d9748f225e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: \n",
      "0.010994641110301018\n",
      "\n",
      "True Response: \n",
      "\n",
      "I have so many issues to address. I have a history of sexual abuse, I’m a breast cancer survivor and I am a lifetime insomniac. I have a long history of depression and I’m beginning to have anxiety. I have low self esteem but I’ve been happily married for almost 35 years. I’ve never had counseling about any of this. Do I have too many issues to address in counseling? \n",
      "\n",
      "Predicted Response: \n",
      "\n",
      "It is no such thing as too many issues for counseling. Many issues are often <UNK> and can all be worked on with some time and patience. <EOS> \n"
     ]
    }
   ],
   "source": [
    "transformer = build_transformer(src_vocab_size=SRC_VOCAB_SIZE, tgt_vocab_size=TGT_VOCAB_SIZE, src_seq_len=MAX_LENGTH, tgt_seq_len=MAX_LENGTH, d_model=D_MODEL, N=8, h=6, dropout=0.1, device=device).to(device)\n",
    "transformer.load_state_dict(torch.load('transformer_model.pth'))\n",
    "transformer.eval()\n",
    "\n",
    "contextSet_tokenized= split_contextSet_tokenized[0].to(device) \n",
    "responseSet_tokenized= split_responseSet_tokenized[0].to(device)\n",
    "label = split_label[0].to(device)\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    for epoch in range(1):\n",
    "        encoder_mask = transformer.make_src_mask(contextSet_tokenized).to(device)\n",
    "        decoder_mask = transformer.make_trg_mask(responseSet_tokenized).to(device)\n",
    "        encoder_input = contextSet_tokenized.to(device)\n",
    "        decoder_input = responseSet_tokenized.to(device)\n",
    "        encoder_output = transformer.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "        decoder_output = transformer.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "        proj_output = transformer.project(decoder_output).to(device) # (B, seq_len, vocab_size)\n",
    "        # Update the weights\n",
    "\n",
    "\n",
    "\n",
    "        logits = proj_output  # Assuming proj_output is your logits tensor\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Apply softmax along the last dimension\n",
    "        predicted_classes = torch.argmax(probabilities, dim=-1)  # Find the index of the class with the highest probability\n",
    "        losses = criterion(proj_output.view(-1, TGT_VOCAB_SIZE), label.view(-1)).to(device)\n",
    "        # Convert tensors to numpy arrays for inspection\n",
    "        #logits_array = logits.numpy()        \n",
    "        probabilities_array = probabilities.cpu().numpy()\n",
    "        predicted_classes_array = predicted_classes.cpu().numpy()\n",
    "        sent = \"\"\n",
    "        for i in predicted_classes_array[7]:\n",
    "            if i<4:\n",
    "                if i == 1:\n",
    "                    sent = sent + \"<UNK>\"\n",
    "                    sent = sent + \" \"\n",
    "                if i == 3:\n",
    "                    sent = sent + \"<EOS>\"\n",
    "                    sent = sent + \" \"\n",
    "                    break\n",
    "                continue\n",
    "            for word in TGT_TOKENS:\n",
    "                if TGT_TOKENS[word] == i:\n",
    "                    sent = sent + word\n",
    "                    sent = sent + \" \"\n",
    "        print(\"\")\n",
    "        print(\"Loss: \")\n",
    "        print(losses.item())\n",
    "        print(\"\")\n",
    "        print(\"True Response: \")\n",
    "        print(\"\")\n",
    "        sent1 = \"\"\n",
    "        for i in contextSet_tokenized[7]:\n",
    "            if i<4:\n",
    "                if i == 1:\n",
    "                    sent1 = sent1 + \"<UNK>\"\n",
    "                    sent1 = sent1 + \" \"\n",
    "                if i == 3:\n",
    "                    sent1 = sent1 + \"<EOS>\"\n",
    "                    sent1 = sent1 + \" \"\n",
    "                    break\n",
    "                continue\n",
    "            for word in SRC_TOKENS:\n",
    "                if SRC_TOKENS[word] == i:\n",
    "                    sent1 = sent1 + word\n",
    "                    sent1 = sent1 + \" \"\n",
    "        print(sent1)\n",
    "        print(\"\")\n",
    "        print(\"Predicted Response: \")\n",
    "        print(\"\")\n",
    "        print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ad19551c-f7e9-4766-b026-b24f642cc521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10439\n",
      "['I', 'start', 'counseling/therapy', 'in', 'a', 'few', 'days', \"(I'm\", 'freaking', 'out)', 'but', 'my', 'main', 'fear', 'is', 'that', \"I'll\", 'cry', 'and', 'embarrass', 'myself,', 'is', 'it', 'something', 'to', 'worry', 'about?']\n",
      "27\n",
      "200\n",
      "It is very common for people to have multiple issues that they want to (and need address in counseling. I have had clients ask that same question and through more exploration, there is often an underlying fear that they be or that they will \"be too much for their therapist.\" I don't know if any of this rings true for you. But, most people have more than one problem in their lives and more often than not, people have numerous significant stressors in their lives. Let's face it, life can be Therapists are completely ready and equipped to handle all of the issues small or large that a client presents in session. Most therapists over the first couple of sessions will help you prioritize the issues you are facing so that you start addressing the issues that are causing you the most distress. You can never have too many issues to address in counseling. All of the issues you mention above can be successfully worked through in counseling. \n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(TGT_VOCAB_SIZE)\n",
    "transformer = build_transformer(src_vocab_size=SRC_VOCAB_SIZE, tgt_vocab_size=TGT_VOCAB_SIZE, src_seq_len=MAX_LENGTH, tgt_seq_len=MAX_LENGTH, d_model=D_MODEL, N=8, h=6, dropout=0.1, device=device).to(device)\n",
    "transformer.load_state_dict(torch.load('transformer_model.pth'))\n",
    "transformer.eval()\n",
    "first = \"I start counseling/therapy in a few days (I'm freaking out) but my main fear is that I'll cry and embarrass myself, is it something to worry about?\"\n",
    "first = first.split()\n",
    "second = \"Do I have too many issues  \"\n",
    "second = second.split()\n",
    "print(first)\n",
    "inference = [first, second]\n",
    "inf_tokens = []\n",
    "decoder_tokens = [[SOS_TOKEN], [SOS_TOKEN]]\n",
    "for example in inference:\n",
    "    tok = []\n",
    "    for word in example:\n",
    "        tok.append(SRC_TOKENS[word])\n",
    "    inf_tokens.append(tok[:])\n",
    "print(len(inf_tokens[0]))\n",
    "for i in range(len(inf_tokens)):\n",
    "    total = 200-len(inf_tokens[i])\n",
    "    for j in range(total):\n",
    "        inf_tokens[i].append(PADDING_TOKEN)\n",
    "\n",
    "for i in range(len(decoder_tokens)):\n",
    "    for j in range(199):\n",
    "        decoder_tokens[i].append(PADDING_TOKEN)\n",
    "print(len(inf_tokens[0]))\n",
    "contextSet_tokenized = []\n",
    "responseSet_tokenized = []\n",
    "contextSet_tokenized = torch.tensor(inf_tokens)\n",
    "responseSet_tokenized = torch.tensor(decoder_tokens)\n",
    "cur_word_idx = 0\n",
    "finished = {}\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    encoder_mask = transformer.make_src_mask(contextSet_tokenized).to(device)\n",
    "    decoder_mask = transformer.make_trg_mask(responseSet_tokenized).to(device)\n",
    "    encoder_input = contextSet_tokenized.to(device)\n",
    "    decoder_input = responseSet_tokenized.to(device)\n",
    "    encoder_output = transformer.encode(encoder_input, encoder_mask)  # (B, seq_len, d_model)\n",
    "\n",
    "    for i in range(MAX_LENGTH - 1):\n",
    "        decoder_output = transformer.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)  # (B, seq_len, d_model)\n",
    "        proj_output = transformer.project(decoder_output).to(device)  # (B, seq_len, vocab_size)\n",
    "        logits = proj_output  # Assuming proj_output is your logits tensor\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Apply softmax along the last dimension\n",
    "        predicted_classes = torch.argmax(probabilities, dim=-1)  # Find the index of the class with the highest probability [[4,76,3,5], [4,67,43, 4]]\n",
    "\n",
    "        for example in range(len(predicted_classes)):\n",
    "            if example not in finished:\n",
    "                decoder_input[example][cur_word_idx + 1] = predicted_classes[example][cur_word_idx]\n",
    "                if decoder_input[example][cur_word_idx + 1] == EOS_TOKEN:\n",
    "                    finished[example] = 1\n",
    "                decoder_mask = transformer.make_trg_mask(decoder_input).to(device)\n",
    "        cur_word_idx += 1\n",
    "\n",
    "        # Convert tensors to numpy arrays for inspection\n",
    "        # logits_array = logits.numpy()\n",
    "        probabilities_array = probabilities.cpu().numpy()\n",
    "        predicted_classes_array = predicted_classes.cpu().numpy()\n",
    "\n",
    "sentence = \"\"\n",
    "\n",
    "for token in decoder_input[1]:\n",
    "    for word in TGT_TOKENS:\n",
    "        if TGT_TOKENS[word] == token:\n",
    "            sentence = sentence + word + \" \"\n",
    "\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3bcfb29-e368-4646-9718-1542f478b11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People do cry in therapy sometimes, but it's not at all necessary to cry in order for most kinds of therapy to be helpful. When you start counseling you don't yet know your counselor very well, so it's normal to keep your feelings in check until you feel comfortable and a bit more relaxed with your counselor and with the situation. Sometimes, though, there are emotions that have been waiting and waiting to finally find someone who will listen with a kind ear. If you feel safe right away in the situation with your counselor, you might just cry in spite of your fears about it. Your therapist is used to people expressing how they feel and will keep strict confidentiality, so even though it's embarrassing, finally experiencing someone truly listening with empathy and kindness may just be worth it. It's okay too to let your counselor know right at the beginning that you're kind of freaked out about getting too emotional in front of another person. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = \"\"\n",
    "for token in decoder_input[0]:\n",
    "    for word in TGT_TOKENS:\n",
    "        if TGT_TOKENS[word] == token:\n",
    "            sentence = sentence + word + \" \"\n",
    "print(sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
