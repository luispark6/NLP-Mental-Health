{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bca86dc3-6a8c-46f6-a249-4a7fdcb4acee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 200, 252])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import copy\n",
    "from torch import nn, Tensor\n",
    "\n",
    "D_MODEL = 252\n",
    "PADDING_TOKEN = 0\n",
    "UNK_TOKEN = 1\n",
    "SOS_TOKEN = 2\n",
    "EOS_TOKEN = 3\n",
    "MAX_LENGTH = 200\n",
    "tokens = json.load(open(\"tokens.txt\"))\n",
    "model = torch.load(\"embedding_model\")\n",
    "file_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"mental_health.csv\")\n",
    "orig_dataset = pd.read_csv(file_path)\n",
    "orig_dataset = orig_dataset.to_numpy()\n",
    "unfiltered_trainingSet =  orig_dataset[np.random.choice(orig_dataset.shape[0], 100, replace=True)] #extract training set\n",
    "trainingSet = []\n",
    "for example in unfiltered_trainingSet:\n",
    "    if type(example[0]) is not float and type(example[1]) is not float:\n",
    "        if len(example[0].split())<200 and len(example[1].split())<200:\n",
    "            trainingSet.append(example)\n",
    "\n",
    "            \n",
    "contextSet = [trainingSet[i][0] for i in range(len(trainingSet))] \n",
    "responseSet = [trainingSet[i][1] for i in range(len(trainingSet))]\n",
    "#tokenizing the context and response set, also 1 is special token for unknown word\n",
    "contextSet_tokenized = [[tokens[word] if word in tokens else UNK_TOKEN for word in example.split()] \n",
    "                         for example in contextSet ]\n",
    "\n",
    "\n",
    "\n",
    "responseSet_tokenized = [[tokens[word] if word in tokens else UNK_TOKEN for word in example.split()] \n",
    "                         for example in responseSet ]\n",
    "\n",
    "label =copy.deepcopy(responseSet_tokenized)\n",
    "#set up special tokens\n",
    "for i in range(len(contextSet_tokenized)):\n",
    "    while len(contextSet_tokenized[i])!= MAX_LENGTH:\n",
    "        contextSet_tokenized[i].append(PADDING_TOKEN)\n",
    "for i in range(len(responseSet_tokenized)):\n",
    "    responseSet_tokenized[i].insert(0, SOS_TOKEN)\n",
    "    label[i].append(EOS_TOKEN)\n",
    "for i in range(len(responseSet_tokenized)):\n",
    "    while len(responseSet_tokenized[i]) != MAX_LENGTH:\n",
    "        responseSet_tokenized[i].append(PADDING_TOKEN)\n",
    "        label[i].append(PADDING_TOKEN)\n",
    "\n",
    "    \n",
    "#convert token to input embedding for context and response set filled with padding\n",
    "#tokens if end of sentence. 0 is special token for padding \n",
    "contextSet_embedding = []        \n",
    "for context in contextSet_tokenized:\n",
    "    contextEmbedding = []\n",
    "    for i in range(MAX_LENGTH):\n",
    "        if i>= len(context):\n",
    "            contextEmbedding.append(model[\"embeddings.weight\"][1])\n",
    "            continue\n",
    "        contextEmbedding.append(model[\"embeddings.weight\"][context[i]])\n",
    "    contextEmbedding = torch.stack(contextEmbedding)\n",
    "    contextSet_embedding.append(contextEmbedding[:])\n",
    "\n",
    "contextSet_embedding = torch.stack(contextSet_embedding)\n",
    "responseSet_embedding = []        \n",
    "for response in responseSet_tokenized:\n",
    "    responseEmbedding = []\n",
    "    for i in range(MAX_LENGTH):\n",
    "        responseEmbedding.append(model[\"embeddings.weight\"][response[i]])\n",
    "    responseEmbedding = torch.stack(responseEmbedding)\n",
    "    responseSet_embedding.append(responseEmbedding[:])\n",
    "\n",
    "contextSet_tokenized = torch.tensor(contextSet_tokenized)\n",
    "responseSet_tokenized = torch.tensor(responseSet_tokenized)\n",
    "\n",
    "responseSet_embedding = torch.stack(responseSet_embedding)\n",
    "print(responseSet_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba420137-3b54-46c9-83a5-84b855b7a7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model = D_MODEL, seq_len = MAX_LENGTH, dropout = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "764d36e8-0be8-45bf-849a-18fca3070ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float)->None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # linear layer for queue\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # liner layer for key\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # linear layer for \n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "                 \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        #d_k is number of embeddings per word for each head\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        # [[[[4,3],[2,1]],[[2, 5], [3,2]]]] * [[[[4,3][2,5]],[[2,1], [3,2]]]] = \n",
    "        #[[[[0.7, 0.3], [0.1, 0.9]], [0.8, 0.2], [0.4, 0.6]]]\n",
    "        #basically saying the first word of the first half of the embedding(for head 1) is related to\n",
    "        #to its own word by 0.7 and related to the first half embedding of the second word by 0.3\n",
    "        #also saying the second word of the first half of the embedding(for head 1) is related to\n",
    "        #to its own word by 0.9 and related to the first half embedding of the first word by 0.1\n",
    "        #essentially a attention matrix for each part of the embedding for the head\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        #if mask is true, then make all appropriate masked attentions scores to very low value\n",
    "        #so that the soft max will ignore their scores\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        #[[[4,3,2,1] , [2,5,3,2]]]\n",
    "        #3rd dimension represents word embeddings of a word\n",
    "        #2nd dimension represent each word embedding in an example\n",
    "        #1st dimension represents each example containing each word embedding\n",
    "        # * Transform the dimensions using .view *\n",
    "        # [[[[4,3],[2,1]],[[2, 5], [3,2]]]]\n",
    "        #4th dimenension represents the size of the split embeddings for each multi-head attention\n",
    "        #3rd dimension represents the number of heads. Note that head times the size of split embeddings equals original embedding dimension\n",
    "        #2nd dimension represents sequence length\n",
    "        #1st dimension represents example lenght\n",
    "        # * Transponse dimensions 1,2 *\n",
    "        # [[[[4,3][2,5]],[[2,1], [3,2]]]]\n",
    "        # 4th dimension represents the size of the split embeddings for each multi-head attention\n",
    "        # 3rd dimension represents the length of the sequence. Contains each word with its associated head\n",
    "        # 2nd dimension represents the number of heads\n",
    "        # 1st dimension is the number of examples\n",
    "        #[[[[word 1 with first half embeddings][word 2 with first half embedding]],[[word 1 with second half], [word 2 with second half]]]]        \n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        #pass combined heads to last linear layer\n",
    "        return self.w_o(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc1610c-d6f6-4ef8-975d-223a4c8ac77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        #the output of the self.linear1 layer is the input of a relu layer\n",
    "        #the output of that is the input of linear layer 2\n",
    "     \n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb762885-8309-40e1-978b-62b8cdacbe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the data for stabilized training\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53b4e705-6736-4c6b-ba6b-6bcf42c4f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "    def forward(self, x, sublayer):\n",
    "        # in the paper, you would typically add x to the sublayer and then normalize the output\n",
    "        #of the sublayer, but in this case, we normalize before passing it in to the sublayer\n",
    "        #the .norm part is the normalizing part in ADD and NORM, and the addition of x is the \n",
    "        #ADD part of ADD and NORM\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89ad844d-9fca-4fbf-9343-e1900f44d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        #encoder block contains 2 residual connections\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        #Passes self attention block to residuals. Residuals will perform the attention block\n",
    "        #and also perform the ADD and NORM\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        #passes the output of the previous residual_connection layer('x'). Then, passes in a feed_forward_block\n",
    "        #Residual connection will perform the feed forward and ADD and NORM\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a51e5746-97a1-468f-83c9-60206377b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        #layers of encoder blocks\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        #perform a forward method on every encoder block. The output of each encoder\n",
    "        #becomes the input of the new encoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        #normalize the final output\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e87e855-5461-4bf9-be13-049368037b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "    \n",
    "    #src mask are for the encoder output. Do not want to attend to padding tokens\n",
    "    #tgt mask are for decoder input. Doesn't let you look into the future\n",
    "    #typically these just always be true I think\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8345ccd-9a72-41f3-a9f7-7c491fb67580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0b49481-6256-440b-a008-d267f62b4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear layer where output represents all words. We are going to soft max this \n",
    "#in the future. \n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c306bd0f-0a09-4729-ab51-c5d139ce52bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed, tgt_embed, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer, src_pad_indx = 0, device = \"cpu\") -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "        self.src_pad_idx = src_pad_indx\n",
    "        self._pad_idx = src_pad_indx\n",
    "        self.device = device\n",
    "    \n",
    "    def make_src_mask(self, src_tokens):\n",
    "        src_mask = (src_tokens != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "    \n",
    "    def make_trg_mask(self, trg_tokens):\n",
    "        N, trg_len = trg_tokens.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        \n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "897029fa-3e85-48b0-b8fd-43172307424d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_transformer(src_embed, tgt_embed, src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048, device = \"cpu\") -> Transformer:\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout).to(device)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout).to(device)\n",
    "    \n",
    "\n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout).to(device)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout).to(device)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout).to(device)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout).to(device)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout).to(device)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout).to(device)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout).to(device)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks)).to(device)\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks)).to(device)\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size).to(device)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer, device=device).to(device)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab0ca881-55df-42a9-978e-9817aca92cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 20.38 GB, other allocations: 35.86 MB, max allowed: 20.40 GB). Tried to allocate 7.25 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m responseSet_embedding\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mencode(encoder_input, encoder_mask) \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mdecode(encoder_output, encoder_mask, decoder_input, decoder_mask) \u001b[38;5;66;03m# (B, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m proj_output \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mproject(decoder_output) \u001b[38;5;66;03m# (B, seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(proj_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4604\u001b[39m), label\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[23], line 36\u001b[0m, in \u001b[0;36mTransformer.decode\u001b[0;34m(self, encoder_output, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoder_output: torch\u001b[38;5;241m.\u001b[39mTensor, src_mask: torch\u001b[38;5;241m.\u001b[39mTensor, tgt: torch\u001b[38;5;241m.\u001b[39mTensor, tgt_mask: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# (batch, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_pos(tgt)\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, encoder_output, src_mask, tgt_mask)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, encoder_output, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, encoder_output, src_mask, tgt_mask):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m----> 8\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x, encoder_output, src_mask, tgt_mask)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, encoder_output, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, encoder_output, src_mask, tgt_mask):\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m0\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention_block(x, x, x, tgt_mask))\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m1\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention_block(x, encoder_output, encoder_output, src_mask))\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m2\u001b[39m](x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_block)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m, in \u001b[0;36mResidualConnection.forward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, sublayer):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# in the paper, you would typically add x to the sublayer and then normalize the output\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#of the sublayer, but in this case, we normalize before passing it in to the sublayer\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#the .norm part is the normalizing part in ADD and NORM, and the addition of x is the \u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#ADD part of ADD and NORM\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sublayer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m, in \u001b[0;36mLayerNormalization.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m std \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mstd(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# (batch, seq_len, 1)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# eps is to prevent dividing by zero or when std is very small\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m (x \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m (std \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 20.38 GB, other allocations: 35.86 MB, max allowed: 20.40 GB). Tried to allocate 7.25 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "transformer = build_transformer(contextSet_embedding, responseSet_embedding, src_vocab_size = 4604, \n",
    "                                tgt_vocab_size = 4604, src_seq_len = MAX_LENGTH, \n",
    "                                tgt_seq_len = MAX_LENGTH, d_model = 252, N =6, h=6, dropout=0.1, device=device).to(device)\n",
    "label = torch.tensor(label).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(transformer.parameters(), eps=1e-9)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    losses = 0\n",
    "    decoder_mask = transformer.make_src_mask(contextSet_tokenized).to(device)\n",
    "    encoder_mask = transformer.make_trg_mask(responseSet_tokenized).to(device)\n",
    "    encoder_input = contextSet_embedding.to(device)\n",
    "    decoder_input = responseSet_embedding.to(device)\n",
    "    encoder_output = transformer.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "    decoder_output = transformer.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "    proj_output = transformer.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "    loss = criterion(proj_output.view(-1, 4604), label.view(-1))\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    losses += loss\n",
    "    print(losses)\n",
    "    #Inside your loop\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        logits = proj_output  # Assuming proj_output is your logits tensor\n",
    "        probabilities = F.softmax(logits, dim=-1)  # Apply softmax along the last dimension\n",
    "        predicted_classes = torch.argmax(probabilities, dim=-1)  # Find the index of the class with the highest probability\n",
    "\n",
    "        # Convert tensors to numpy arrays for inspection\n",
    "        logits_array = logits.numpy()\n",
    "        probabilities_array = probabilities.numpy()\n",
    "        predicted_classes_array = predicted_classes.numpy()        \n",
    "        sent = \"\"\n",
    "        for i in predicted_classes_array[0]:\n",
    "            if i<5:\n",
    "                continue\n",
    "            for word in tokens:\n",
    "                if tokens[word] == i:\n",
    "                    sent = sent + word\n",
    "                    sent = sent + \" \"\n",
    "        print(\"\")\n",
    "        print(\"True Response: \")\n",
    "        print(\"\")\n",
    "        print(responseSet[0])\n",
    "        print(\"\")\n",
    "        print(\"Predicted Response: \")\n",
    "        print(\"\")\n",
    "        print(sent)\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
